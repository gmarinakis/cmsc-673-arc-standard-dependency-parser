{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28f7eebb530>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"universal_dependencies\"\n",
    "ud_config = get_dataset_config_names(name)\n",
    "ud_ewt_train = load_dataset(name, 'en_ewt', split=\"train\")\n",
    "ud_ewt_dev = load_dataset(name, 'en_ewt', split=\"validation\")\n",
    "ud_ewt_test = load_dataset(name, 'en_ewt', split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 'weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000-0035',\n",
       " 'text': 'The situation in Iraq is only going to get better this way.',\n",
       " 'tokens': ['The',\n",
       "  'situation',\n",
       "  'in',\n",
       "  'Iraq',\n",
       "  'is',\n",
       "  'only',\n",
       "  'going',\n",
       "  'to',\n",
       "  'get',\n",
       "  'better',\n",
       "  'this',\n",
       "  'way',\n",
       "  '.'],\n",
       " 'lemmas': ['the',\n",
       "  'situation',\n",
       "  'in',\n",
       "  'Iraq',\n",
       "  'be',\n",
       "  'only',\n",
       "  'go',\n",
       "  'to',\n",
       "  'get',\n",
       "  'better',\n",
       "  'this',\n",
       "  'way',\n",
       "  '.'],\n",
       " 'upos': [8, 0, 2, 10, 17, 14, 16, 7, 16, 6, 8, 0, 1],\n",
       " 'xpos': ['DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'VBZ',\n",
       "  'RB',\n",
       "  'VBG',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'JJR',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  '.'],\n",
       " 'feats': [\"{'Definite': 'Def', 'PronType': 'Art'}\",\n",
       "  \"{'Number': 'Sing'}\",\n",
       "  'None',\n",
       "  \"{'Number': 'Sing'}\",\n",
       "  \"{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\",\n",
       "  'None',\n",
       "  \"{'Tense': 'Pres', 'VerbForm': 'Part'}\",\n",
       "  'None',\n",
       "  \"{'VerbForm': 'Inf'}\",\n",
       "  \"{'Degree': 'Cmp'}\",\n",
       "  \"{'Number': 'Sing', 'PronType': 'Dem'}\",\n",
       "  \"{'Number': 'Sing'}\",\n",
       "  'None'],\n",
       " 'head': ['2', '7', '4', '2', '7', '7', '0', '9', '7', '9', '12', '10', '7'],\n",
       " 'deprel': ['det',\n",
       "  'nsubj',\n",
       "  'case',\n",
       "  'nmod',\n",
       "  'aux',\n",
       "  'advmod',\n",
       "  'root',\n",
       "  'mark',\n",
       "  'xcomp',\n",
       "  'xcomp',\n",
       "  'det',\n",
       "  'obj',\n",
       "  'punct'],\n",
       " 'deps': [\"[('det', 2)]\",\n",
       "  \"[('nsubj', 7), ('nsubj:xsubj', 9), ('nsubj:xsubj', 10)]\",\n",
       "  \"[('case', 4)]\",\n",
       "  \"[('nmod:in', 2)]\",\n",
       "  \"[('aux', 7)]\",\n",
       "  \"[('advmod', 7)]\",\n",
       "  \"[('root', 0)]\",\n",
       "  \"[('mark', 9)]\",\n",
       "  \"[('xcomp', 7)]\",\n",
       "  \"[('xcomp', 9)]\",\n",
       "  \"[('det', 12)]\",\n",
       "  \"[('obj', 10)]\",\n",
       "  \"[('punct', 7)]\"],\n",
       " 'misc': ['None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  \"{'SpaceAfter': 'No'}\",\n",
       "  'None']}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ud_ewt_train[34] is good default\n",
    "ud_ewt_train[34]['tokens']\n",
    "ud_ewt_train[34]['upos'] #Part of speech tag as an integer index of [\"NOUN\",\"PUNCT\",\"ADP\",\"NUM\",\"SYM\",\"SCONJ\",\"ADJ\",\"PART\",\"DET\",\"CCONJ\",\"PROPN\",\"PRON\",\"X\",\"_\",\"ADV\",\"INTJ\",\"VERB\",\"AUX\"]\n",
    "upos_map = [\"NOUN\",\"PUNCT\",\"ADP\",\"NUM\",\"SYM\",\"SCONJ\",\"ADJ\",\"PART\",\"DET\",\"CCONJ\",\"PROPN\",\"PRON\",\"X\",\"_\",\"ADV\",\"INTJ\",\"VERB\",\"AUX\"]\n",
    "ud_ewt_train[34]['xpos'] #Other POS tag, might be more accurate?\n",
    "ud_ewt_train[34]['deprel'] #arc labels by themselves. https://universaldependencies.org/docs/u/dep/index.html\n",
    "ud_ewt_train[34]['deps'] #arc labels and the index they interact with. NOTE: index starts at one, ROOT is assumed\n",
    "ud_ewt_train[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentence from exmaple in chen and manning\n",
    "test_sent = {'text': 'He has really good control.',\n",
    "             'tokens': ['He', 'has', 'good', 'control', '.'],\n",
    "             'upos': [11, 16, 6, 0, 1], #probably need to use xpos,\n",
    "             'xpos': ['PRP', 'VBZ', 'JJ', 'NN', '.'],\n",
    "             'deprel': ['nsubj', 'root', 'amod', 'dobj', 'punct'],\n",
    "             'deps': [\"[('nsubj', 2)]\", \"[('root', 0)]\", \"[('amod', 4)]\", \"[('dobj', 2)]\", \"[('punct', 2)]\"] #these are strings for some reason\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEFT-ARC(l): adds an arc s1 → s2 with label l and removes s2 from the stack. Precondition: |s| ≥ 2. from Chen and Manning\n",
    "def left_arc(stack, buffer, arcs, dep, print_output=True):\n",
    "    if len(stack) < 2:\n",
    "        print(\"[@] LEFT-ARC called incorrectly, check stack size\")\n",
    "        # return\n",
    "    elif stack[-2] == \"[ROOT]\":\n",
    "        print(\"[@] LEFT-ARC called incorrectly, tried to add depepndency to ROOT\")\n",
    "    else:\n",
    "        arcs.append((dep, (stack[-1],stack.pop(-2))))\n",
    "    if print_output:\n",
    "        print(\"Stack: \", stack, end=\" | \")\n",
    "        print(\"Buffer: \", buffer, end=\" | \")\n",
    "        print(\"Arcs:\", arcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RIGHT-ARC(l): adds an arc s2 → s1 with label l and removes s1 from the stack. Precondition: |s| ≥ 2. from Chen and Manning\n",
    "def right_arc(stack, buffer, arcs, dep, print_output=True):\n",
    "    if len(stack) < 2:\n",
    "        print(\"[@] RIGHT-ARC called incorrectly, check stack size\")\n",
    "        # return\n",
    "    else:\n",
    "        arcs.append((dep, (stack[-2],stack.pop(-1))))\n",
    "    if print_output:\n",
    "        print(\"Stack: \", stack, end=\" | \")\n",
    "        print(\"Buffer: \", buffer, end=\" | \")\n",
    "        print(\"Arcs:\", arcs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHIFT: moves b1 from the buffer to the stack. Precondition: |b| ≥ 1. from Chen and Manning\n",
    "def shift(stack, buffer, arcs, print_output=True):\n",
    "    if len(buffer) < 1:\n",
    "        print(\"[@] SHIFT called incorrectly, check buffer size\")\n",
    "        # return\n",
    "    else:\n",
    "        stack.append(buffer.pop(0))\n",
    "    if print_output:\n",
    "        print(\"Stack: \", stack, end=\" | \")\n",
    "        print(\"Buffer: \", buffer, end=\" | \")\n",
    "        print(\"Arcs:\", arcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack:  ['[ROOT]', 'He'] | Buffer:  ['has', 'good', 'control', '.'] | Arcs: []\n",
      "Stack:  ['[ROOT]', 'He', 'has'] | Buffer:  ['good', 'control', '.'] | Arcs: []\n",
      "Stack:  ['[ROOT]', 'has'] | Buffer:  ['good', 'control', '.'] | Arcs: [('nsubj', ('has', 'He'))]\n",
      "Stack:  ['[ROOT]', 'has', 'good'] | Buffer:  ['control', '.'] | Arcs: [('nsubj', ('has', 'He'))]\n",
      "Stack:  ['[ROOT]', 'has', 'good', 'control'] | Buffer:  ['.'] | Arcs: [('nsubj', ('has', 'He'))]\n",
      "Stack:  ['[ROOT]', 'has', 'control'] | Buffer:  ['.'] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good'))]\n",
      "Stack:  ['[ROOT]', 'has'] | Buffer:  ['.'] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good')), ('dobj', ('has', 'control'))]\n",
      "Stack:  ['[ROOT]', 'has', '.'] | Buffer:  [] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good')), ('dobj', ('has', 'control'))]\n",
      "Stack:  ['[ROOT]', 'has'] | Buffer:  [] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good')), ('dobj', ('has', 'control')), ('punct', ('has', '.'))]\n",
      "Stack:  ['[ROOT]'] | Buffer:  [] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good')), ('dobj', ('has', 'control')), ('punct', ('has', '.')), ('root', ('[ROOT]', 'has'))]\n"
     ]
    }
   ],
   "source": [
    "#Goal is to predict the correct transitions at each step aka predict the deps and timing of deps correctly\n",
    "#Sample of \"correct\" transitions for \"He has good control.\"\n",
    "test_stack = [\"[ROOT]\"]\n",
    "test_buffer = copy.deepcopy(test_sent['tokens'])\n",
    "# buffer = [(test_sent['tokens'][i], test_sent['tokens'][i]) for i in range(len(test_sent['tokens']))] #include xpos in buffer in the future\n",
    "test_arcs = []\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "left_arc(test_stack, test_buffer, test_arcs, 'nsubj')\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "left_arc(test_stack, test_buffer, test_arcs, 'amod')\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'dobj')\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'punct')\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nsubj', ('has', 'He')),\n",
       " ('amod', ('control', 'good')),\n",
       " ('dobj', ('has', 'control')),\n",
       " ('punct', ('has', '.')),\n",
       " ('root', ('[ROOT]', 'has'))]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nsubj', ('has', 'He')),\n",
       " ('root', ('[ROOT]', 'has')),\n",
       " ('amod', ('control', 'good')),\n",
       " ('dobj', ('has', 'control')),\n",
       " ('punct', ('has', '.'))]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare to arcs in UD format\n",
    "# [\"[('nsubj', 2)]\", \"[('root', 0)]\", \"[('amod', 4)]\", \"[('dobj', 2)]\", \"[('punct', 2)]\"]\n",
    "valid_arcs = []\n",
    "for i in range(len(test_sent['deps'])):\n",
    "    curr_dep = ast.literal_eval(test_sent['deps'][i])[0]\n",
    "    valid_arcs.append((curr_dep[0],((([\"[ROOT]\"] + test_sent['tokens'])[curr_dep[1]]), test_sent['tokens'][i])))\n",
    "valid_arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('det', (('situation', 'NN'), ('The', 'DT'))),\n",
       " ('nsubj', (('going', 'VBG'), ('situation', 'NN'))),\n",
       " ('case', (('Iraq', 'NNP'), ('in', 'IN'))),\n",
       " ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))),\n",
       " ('aux', (('going', 'VBG'), ('is', 'VBZ'))),\n",
       " ('advmod', (('going', 'VBG'), ('only', 'RB'))),\n",
       " ('root', (('[ROOT]', 'ROOT'), ('going', 'VBG'))),\n",
       " ('mark', (('get', 'VB'), ('to', 'TO'))),\n",
       " ('xcomp', (('going', 'VBG'), ('get', 'VB'))),\n",
       " ('xcomp', (('get', 'VB'), ('better', 'JJR'))),\n",
       " ('det', (('way', 'NN'), ('this', 'DT'))),\n",
       " ('obj', (('better', 'JJR'), ('way', 'NN'))),\n",
       " ('punct', (('going', 'VBG'), ('.', '.')))]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent2 = ud_ewt_train[34]\n",
    "valid_arcs2 = []\n",
    "for i in range(len(test_sent2['deps'])):\n",
    "    curr_dep = ast.literal_eval(test_sent2['deps'][i])[0]\n",
    "    valid_arcs2.append((test_sent2['deprel'][i],(((([\"[ROOT]\"] + test_sent2['tokens'])[curr_dep[1]]), ([\"ROOT\"] + test_sent2['xpos'])[curr_dep[1]]), (test_sent2['tokens'][i], test_sent2['xpos'][i])))) #use deprel for the arc name. ignoring enhanced depependencies for now\n",
    "valid_arcs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([i in valid_arcs for i in test_arcs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To generate training data, need to reverse the valid arcs into operations which produce them\n",
    "#The \"training oracle\" as described in slp3 which determines which transition to do. return a list of transitions\n",
    "#sentence is UD format sentence\n",
    "#Stack/Buffer format: List of tuple with arc label and tuple of (word, POS) tuples in parent, child order\n",
    "#[ ( arc-label, ( (parent-word, parent-POS), (child-word, child-POS) ) ), ... ]\n",
    "def training_oracle(sentence):\n",
    "    stack = [(\"[ROOT]\", \"ROOT\")]\n",
    "    # buffer = copy.deepcopy(sentence['tokens'])\n",
    "    buffer = [(sentence['tokens'][i], sentence['xpos'][i]) for i in range(len(sentence['tokens']))]\n",
    "    arcs = []\n",
    "    transitions = [] #what we return\n",
    "    labeled_arcs = []\n",
    "    for i in range(len(sentence['deps'])):\n",
    "        curr_dep = ast.literal_eval(sentence['deps'][i])[0]\n",
    "        labeled_arcs.append((sentence['deprel'][i],(((([\"[ROOT]\"] + sentence['tokens'])[curr_dep[1]]), ([\"ROOT\"] + sentence['xpos'])[curr_dep[1]]), (sentence['tokens'][i], sentence['xpos'][i]))))\n",
    "    labeled_arcs_copy = copy.deepcopy(labeled_arcs)\n",
    "    unlabeled_arcs = [i[1] for i in labeled_arcs]\n",
    "    # print(labeled_arcs_copy)\n",
    "    for i in range(2*len(sentence['tokens'])): #2N transitions\n",
    "        if len(stack) >= 2:\n",
    "            # print([j[0] for j in unlabeled_arcs])\n",
    "            if (stack[-1],stack[-2]) in unlabeled_arcs:\n",
    "                arc_label = labeled_arcs.pop(unlabeled_arcs.index((stack[-1],stack[-2])))[0]\n",
    "                transitions.append(\"left-arc \" + arc_label)\n",
    "                unlabeled_arcs.remove((stack[-1],stack[-2]))\n",
    "                left_arc(stack, buffer, arcs, arc_label)\n",
    "            elif (stack[-2],stack[-1]) in unlabeled_arcs and stack[-1] not in [j[0] for j in unlabeled_arcs]: #all of the dependents of the word at the top of the stack must already be assign before right arc,\n",
    "                arc_label = labeled_arcs.pop(unlabeled_arcs.index((stack[-2],stack[-1])))[0]\n",
    "                transitions.append(\"right-arc \" + arc_label)\n",
    "                unlabeled_arcs.remove((stack[-2],stack[-1]))\n",
    "                right_arc(stack, buffer, arcs, arc_label)\n",
    "        if len(transitions) <= i: #if neither arc transition has been done do shift\n",
    "            transitions.append(\"shift\")\n",
    "            shift(stack, buffer, arcs)\n",
    "    print(\"All generated arcs are in original deps list:\", all([i in labeled_arcs_copy for i in arcs]))\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack:  [('[ROOT]', 'ROOT'), ('He', 'PRP')] | Buffer:  [('has', 'VBZ'), ('good', 'JJ'), ('control', 'NN'), ('.', '.')] | Arcs: []\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('He', 'PRP'), ('has', 'VBZ')] | Buffer:  [('good', 'JJ'), ('control', 'NN'), ('.', '.')] | Arcs: []\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ')] | Buffer:  [('good', 'JJ'), ('control', 'NN'), ('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('good', 'JJ')] | Buffer:  [('control', 'NN'), ('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('good', 'JJ'), ('control', 'NN')] | Buffer:  [('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('control', 'NN')] | Buffer:  [('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ')] | Buffer:  [('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('dobj', (('has', 'VBZ'), ('control', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('.', '.')] | Buffer:  [] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('dobj', (('has', 'VBZ'), ('control', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ')] | Buffer:  [] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('dobj', (('has', 'VBZ'), ('control', 'NN'))), ('punct', (('has', 'VBZ'), ('.', '.')))]\n",
      "Stack:  [('[ROOT]', 'ROOT')] | Buffer:  [] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('dobj', (('has', 'VBZ'), ('control', 'NN'))), ('punct', (('has', 'VBZ'), ('.', '.'))), ('root', (('[ROOT]', 'ROOT'), ('has', 'VBZ')))]\n",
      "All generated arcs are in original deps list: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['shift',\n",
       " 'shift',\n",
       " 'left-arc nsubj',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc amod',\n",
       " 'right-arc dobj',\n",
       " 'shift',\n",
       " 'right-arc punct',\n",
       " 'right-arc root']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_oracle(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack:  [('[ROOT]', 'ROOT'), ('The', 'DT')] | Buffer:  [('situation', 'NN'), ('in', 'IN'), ('Iraq', 'NNP'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: []\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('The', 'DT'), ('situation', 'NN')] | Buffer:  [('in', 'IN'), ('Iraq', 'NNP'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: []\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN')] | Buffer:  [('in', 'IN'), ('Iraq', 'NNP'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('in', 'IN')] | Buffer:  [('Iraq', 'NNP'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('in', 'IN'), ('Iraq', 'NNP')] | Buffer:  [('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('Iraq', 'NNP')] | Buffer:  [('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN')] | Buffer:  [('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('is', 'VBZ')] | Buffer:  [('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('is', 'VBZ'), ('only', 'RB')] | Buffer:  [('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG')] | Buffer:  [('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('is', 'VBZ'), ('going', 'VBG')] | Buffer:  [('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('going', 'VBG')] | Buffer:  [('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG')] | Buffer:  [('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('to', 'TO')] | Buffer:  [('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB')] | Buffer:  [('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB')] | Buffer:  [('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR')] | Buffer:  [('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT')] | Buffer:  [('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR'), ('way', 'NN')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('.', '.')] | Buffer:  [] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG')] | Buffer:  [] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB'))), ('punct', (('going', 'VBG'), ('.', '.')))]\n",
      "Stack:  [('[ROOT]', 'ROOT')] | Buffer:  [] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB'))), ('punct', (('going', 'VBG'), ('.', '.'))), ('root', (('[ROOT]', 'ROOT'), ('going', 'VBG')))]\n",
      "All generated arcs are in original deps list: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['shift',\n",
       " 'shift',\n",
       " 'left-arc det',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc case',\n",
       " 'right-arc nmod',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc advmod',\n",
       " 'left-arc aux',\n",
       " 'left-arc nsubj',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc mark',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc det',\n",
       " 'right-arc obj',\n",
       " 'right-arc xcomp',\n",
       " 'right-arc xcomp',\n",
       " 'shift',\n",
       " 'right-arc punct',\n",
       " 'right-arc root']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_oracle(ud_ewt_train[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Featureization. We use sets of elements Sw St and Sl as described in 3.1 of Chen and Manning which can be combined to create features\n",
    "def featurize_configuration(stack, buffer, arcs):\n",
    "    S_w = {'s1' : 'NULL', 's2': 'NULL', 's3': 'NULL', 'b1': 'NULL', 'b2': 'NULL', 'b3': 'NULL',\n",
    "           'lc1s1' : 'NULL', 'lc2s1' : 'NULL', 'lc1s2' : 'NULL', 'lc2s2' : 'NULL', 'rc1s1' : 'NULL', 'rc2s1' : 'NULL', 'rc1s2' : 'NULL', 'rc2s2' : 'NULL', #lc1s1 is leftmost child of s1, lc2s1 is second leftmost\n",
    "           'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'} #lc1lc1 is leftmost child of leftmost children\n",
    "    S_t = {'s1' : 'NULL', 's2': 'NULL', 's3': 'NULL', 'b1': 'NULL', 'b2': 'NULL', 'b3': 'NULL',\n",
    "           'lc1s1' : 'NULL', 'lc2s1' : 'NULL', 'lc1s2' : 'NULL', 'lc2s2' : 'NULL', 'rc1s1' : 'NULL', 'rc2s1' : 'NULL', 'rc1s2' : 'NULL', 'rc2s2' : 'NULL',\n",
    "           'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'}\n",
    "    S_l = {'lc1s1' : 'NULL', 'lc2s1' : 'NULL', 'lc1s2' : 'NULL', 'lc2s2' : 'NULL', 'rc1s1' : 'NULL', 'rc2s1' : 'NULL', 'rc1s2' : 'NULL', 'rc2s2' : 'NULL',\n",
    "           'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'}\n",
    "    \n",
    "    #s1 =============================================================\n",
    "    if len(stack) >= 1:\n",
    "       S_w['s1'] = stack[-1][0]\n",
    "       S_t['s1'] = stack[-1][1]\n",
    "       parent_list = [j[0] for j in [i[1] for i in arcs]]\n",
    "       arcs_copy = copy.deepcopy(arcs)\n",
    "       \n",
    "       #first leftmost child\n",
    "      #  print(parent_list)\n",
    "       if stack[-1] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = parent_list.index(stack[-1])\n",
    "           lc1_arc = arcs_copy[idx]\n",
    "          #  print(lc1_arc)\n",
    "           S_w['lc1s1'] = lc1_arc[1][1][0]\n",
    "           S_t['lc1s1'] = lc1_arc[1][1][1]\n",
    "           S_l['lc1s1'] = lc1_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "           #leftmost child of leftmost child\n",
    "           if lc1_arc[1][1] in parent_list:\n",
    "              idx = parent_list.index(lc1_arc[1][1])\n",
    "              lc1lc1_arc = arcs_copy[idx]\n",
    "              S_w['lc1lc1s1'] = lc1lc1_arc[1][1][0]\n",
    "              S_t['lc1lc1s1'] = lc1lc1_arc[1][1][1]\n",
    "              S_l['lc1lc1s1'] = lc1lc1_arc[0]\n",
    "              parent_list.pop(idx)\n",
    "              arcs_copy.pop(idx)\n",
    "      #  print(S_w)\n",
    "       #first rightmost child       \n",
    "       if stack[-1] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = len(parent_list) - parent_list[-1::-1].index(stack[-1]) - 1 #index of last occurence list slicing nonsense\n",
    "           rc1_arc = arcs_copy[idx] \n",
    "           S_w['rc1s1'] = rc1_arc[1][1][0]\n",
    "           S_t['rc1s1'] = rc1_arc[1][1][1]\n",
    "           S_l['rc1s1'] = rc1_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "          #  print(parent_list)\n",
    "           #rightmost child of rightmost child\n",
    "           if rc1_arc[1][1] in parent_list:\n",
    "              idx = len(parent_list) - parent_list[-1::-1].index(rc1_arc[1][1]) - 1\n",
    "              rc1rc1_arc = arcs_copy[idx]\n",
    "              S_w['rc1rc1s1'] = rc1rc1_arc[1][1][0]\n",
    "              S_t['rc1rc1s1'] = rc1rc1_arc[1][1][1]\n",
    "              S_l['rc1rc1s1'] = rc1rc1_arc[0]\n",
    "              parent_list.pop(idx)\n",
    "              arcs_copy.pop(idx)\n",
    "       \n",
    "       #second leftmost child\n",
    "       if stack[-1] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = parent_list.index(stack[-1])\n",
    "           lc2_arc = arcs_copy[idx]\n",
    "           S_w['lc2s1'] = lc2_arc[1][1][0]\n",
    "           S_t['lc2s1'] = lc2_arc[1][1][1]\n",
    "           S_l['lc2s1'] = lc2_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "       \n",
    "       #second rightmost child       \n",
    "       if stack[-1] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = len(parent_list) - parent_list[-1::-1].index(stack[-1]) - 1 #index of last occurence list slicing nonsense\n",
    "           rc2_arc = arcs_copy[idx] \n",
    "           S_w['rc2s1'] = rc2_arc[1][1][0]\n",
    "           S_t['rc2s1'] = rc2_arc[1][1][1]\n",
    "           S_l['rc2s1'] = rc2_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "       \n",
    "       #s2 =================================================================\n",
    "       if len(stack) >= 2:       \n",
    "         S_w['s2'] = stack[-2][0]\n",
    "         S_t['s2'] = stack[-2][1]\n",
    "           \n",
    "         #first leftmost child\n",
    "         if stack[-2] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = parent_list.index(stack[-2])\n",
    "           lc1_arc = arcs_copy[idx]\n",
    "       #     print(lc1_arc)\n",
    "           S_w['lc1s2'] = lc1_arc[1][1][0]\n",
    "           S_t['lc1s2'] = lc1_arc[1][1][1]\n",
    "           S_l['lc1s2'] = lc1_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "           #leftmost child of leftmost child\n",
    "           if lc1_arc[1][1] in parent_list:\n",
    "              idx = parent_list.index(lc1_arc[1][1])\n",
    "              lc1lc1_arc = arcs_copy[idx]\n",
    "              S_w['lc1lc1s2'] = lc1lc1_arc[1][1][0]\n",
    "              S_t['lc1lc1s2'] = lc1lc1_arc[1][1][1]\n",
    "              S_l['lc1lc1s2'] = lc1lc1_arc[0]\n",
    "              parent_list.pop(idx)\n",
    "              arcs_copy.pop(idx)\n",
    "       \n",
    "         #first rightmost child       \n",
    "         if stack[-2] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = len(parent_list) - parent_list[-1::-1].index(stack[-2]) - 1 #index of last occurence list slicing nonsense\n",
    "           rc1_arc = arcs_copy[idx] \n",
    "           S_w['rc1s2'] = rc1_arc[1][1][0]\n",
    "           S_t['rc1s2'] = rc1_arc[1][1][1]\n",
    "           S_l['rc1s2'] = rc1_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "           #rightmost child of rightmost child\n",
    "           if rc1_arc[1][1] in parent_list:\n",
    "              idx = len(parent_list) - parent_list[-1::-1].index(rc1_arc[1][1]) - 1\n",
    "              rc1rc1_arc = arcs_copy[idx]\n",
    "              S_w['rc1rc1s2'] = rc1rc1_arc[1][1][0]\n",
    "              S_t['rc1rc1s2'] = rc1rc1_arc[1][1][1]\n",
    "              S_l['rc1rc1s2'] = rc1rc1_arc[0]\n",
    "              parent_list.pop(idx)\n",
    "              arcs_copy.pop(idx)\n",
    "       \n",
    "         #second leftmost child\n",
    "         if stack[-2] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = parent_list.index(stack[-2])\n",
    "           lc2_arc = arcs_copy[idx]\n",
    "           S_w['lc2s2'] = lc2_arc[1][1][0]\n",
    "           S_t['lc2s2'] = lc2_arc[1][1][1]\n",
    "           S_l['lc2s2'] = lc2_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "       \n",
    "         #second rightmost child       \n",
    "         if stack[-2] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = len(parent_list) - parent_list[-1::-1].index(stack[-2]) - 1 #index of last occurence list slicing nonsense\n",
    "           rc2_arc = arcs_copy[idx] \n",
    "           S_w['rc2s2'] = rc2_arc[1][1][0]\n",
    "           S_t['rc2s2'] = rc2_arc[1][1][1]\n",
    "           S_l['rc2s2'] = rc2_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "           \n",
    "         if len(stack) >= 3:\n",
    "           S_w['s3'] = stack[-3][0]\n",
    "           S_t['s3'] = stack[-3][1]\n",
    "\n",
    "    if len(buffer) >= 1:\n",
    "       S_w['b1'] = buffer[-1][0]\n",
    "       S_t['b1'] = buffer[-1][1]\n",
    "       if len(buffer) >= 2:       \n",
    "           S_w['b2'] = buffer[-2][0]\n",
    "           S_t['b2'] = buffer[-2][1]\n",
    "           if len(buffer) >= 3:\n",
    "              S_w['b3'] = buffer[-3][0]\n",
    "              S_t['b3'] = buffer[-3][1]\n",
    "    \n",
    "    return S_w, S_t, S_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack:  [('[ROOT]', 'NULL'), ('has', 'VBZ')] | Buffer:  [] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('dobj', (('has', 'VBZ'), ('control', 'NN'))), ('punct', (('has', 'VBZ'), ('.', '.')))]\n",
      "({'s1': 'has', 's2': '[ROOT]', 's3': 'NULL', 'b1': 'NULL', 'b2': 'NULL', 'b3': 'NULL', 'lc1s1': 'He', 'lc2s1': 'control', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': '.', 'rc2s1': 'NULL', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'}, {'s1': 'VBZ', 's2': 'NULL', 's3': 'NULL', 'b1': 'NULL', 'b2': 'NULL', 'b3': 'NULL', 'lc1s1': 'PRP', 'lc2s1': 'NN', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': '.', 'rc2s1': 'NULL', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'}, {'lc1s1': 'nsubj', 'lc2s1': 'dobj', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': 'punct', 'rc2s1': 'NULL', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'})\n"
     ]
    }
   ],
   "source": [
    "#Testing the feature list\n",
    "test_stack = [(\"[ROOT]\", \"NULL\")]\n",
    "test_buffer = [(test_sent['tokens'][i], test_sent['xpos'][i]) for i in range(len(test_sent['tokens']))]\n",
    "test_arcs = []\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "left_arc(test_stack, test_buffer, test_arcs, 'nsubj', False)\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "left_arc(test_stack, test_buffer, test_arcs, 'amod', False)\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'dobj', False)\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'punct')\n",
    "print(featurize_configuration(test_stack, test_buffer, test_arcs))\n",
    "# right_arc(test_stack, test_buffer, test_arcs, 'root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'s1': 'going', 's2': '[ROOT]', 's3': 'NULL', 'b1': '.', 'b2': 'NULL', 'b3': 'NULL', 'lc1s1': 'only', 'lc2s1': 'is', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': 'get', 'rc2s1': 'situation', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'better', 'rc1rc1s2': 'NULL'}, {'s1': 'VBG', 's2': 'ROOT', 's3': 'NULL', 'b1': '.', 'b2': 'NULL', 'b3': 'NULL', 'lc1s1': 'RB', 'lc2s1': 'VBZ', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': 'VB', 'rc2s1': 'NN', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'JJR', 'rc1rc1s2': 'NULL'}, {'lc1s1': 'advmod', 'lc2s1': 'aux', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': 'xcomp', 'rc2s1': 'nsubj', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'xcomp', 'rc1rc1s2': 'NULL'})\n"
     ]
    }
   ],
   "source": [
    "test_stack3 = [('[ROOT]', 'ROOT'), ('going', 'VBG')] \n",
    "test_buffer3 = [('.', '.')]\n",
    "test_arcs3 = [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB')))]\n",
    "print(featurize_configuration(test_stack3, test_buffer3, test_arcs3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency list of training vocab\n",
    "train_vocab_en = {}\n",
    "for i in ud_ewt_train:\n",
    "    for j in i['tokens']:\n",
    "        if j not in train_vocab_en:\n",
    "            train_vocab_en[j] = 1\n",
    "        else:\n",
    "            train_vocab_en[j] += 1\n",
    "train_vocab_en = dict(sorted(train_vocab_en.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13654"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use pretrained word embeddings from wikipedia2vec. I recognize that the vocabulary is needlessly large, but why not\n",
    "file = open(\"enwiki_20180420_win10_100d.txt\", \"r\", encoding='utf-8')\n",
    "#nn.embedding uses indexes to map words, so there will be a dictionary of {word:index} and a dictionary of {index:embeddings}\n",
    "#the _all dicts contain all of the embeddings as initialized, useful for referencing only the prtrained embedding values. tokens not in vocab won't be modified anyways\n",
    "word_indexes_en = {}\n",
    "word_indexes_all_en = {}\n",
    "word_embeddings_en = {} \n",
    "word_embeddings_all_en = {}\n",
    "#word_embeddings_en = nn.Embedding(num_embeddings=4530030, embedding_dim=100)\n",
    "skip_first = True\n",
    "has_next=True #use a while with boolean so I can continue through UnicodeDecodeErros and skip weird characters which arent in the vocab anyways\n",
    "current_index_all = 0\n",
    "current_index = 0\n",
    "while has_next:\n",
    "    if not skip_first:\n",
    "        try:\n",
    "            line = file.readline()\n",
    "            if line != \"\":\n",
    "                if not (\"ENTITY/\" in line and \"_\" in line): #We dont care about entities // We can try adding single token entities since most of them are proper nouns, can differnciate by case\n",
    "                    current_line = line.split()\n",
    "                    #print(current_line)\n",
    "                    if current_line != []:\n",
    "                        if \"ENTITY/\" in current_line[0]:\n",
    "                            current_token = current_line[0][current_line[0].index(\"/\")+1:]\n",
    "                        else:\n",
    "                            current_token = current_line[0]\n",
    "                        #print(current_token)\n",
    "                        current_embeds = []\n",
    "                        if len(current_line) == 101: #some words in the pretrained embeddings have a duplication with a space in them (ex. 8_Flora vs 8 Flora, Channel_4 vs Channel 4). Ignore the ones with a space\n",
    "                            for i in current_line[1:]:\n",
    "                                    current_embeds.append(float(i))\n",
    "                            word_indexes_all_en[current_token] = current_index_all\n",
    "                            word_embeddings_all_en[current_index_all] = torch.FloatTensor(current_embeds)\n",
    "                            if current_token in train_vocab_en:\n",
    "                                word_indexes_en[current_token] = current_index\n",
    "                                word_embeddings_en[current_index] = torch.FloatTensor(current_embeds)\n",
    "                                current_index += 1\n",
    "                            current_index_all += 1\n",
    "            else:\n",
    "                has_next=False\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    else: #first line is skipped\n",
    "        file.readline()\n",
    "        skip_first = False\n",
    "file.close()\n",
    "current_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11445 friendlier 11445\n",
      "11446 Confirmation 11446\n",
      "11447 Butcher 11448\n",
      "11448 chairpersons 11449\n",
      "\n",
      "225 2015 11447\n"
     ]
    }
   ],
   "source": [
    "#why does this happen???\n",
    "count = 0\n",
    "for i in word_indexes_en:\n",
    "    if count >= 11445 and count <= 11448:\n",
    "        print(count, i, word_indexes_en[i])\n",
    "    count += 1\n",
    "print(\"\")\n",
    "magic_11447 = '2015'\n",
    "print([i for i in word_indexes_en].index(magic_11447), magic_11447, word_indexes_en[magic_11447])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20326"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomly initialize non intialized embeddings\n",
    "#uses current index and current index all from previous block\n",
    "non_init_word_indexes = {}\n",
    "non_init_word_embeds = {}\n",
    "non_init_word_embeds_all = {}\n",
    "train_vocab_en[\"NULL\"] = 0 #add NULL token\n",
    "for i in train_vocab_en:\n",
    "    if i not in word_indexes_en:\n",
    "        if i not in non_init_word_indexes:\n",
    "            non_init_word_indexes[i] = current_index\n",
    "            init_value = (-0.01 - 0.01) * torch.rand(100) + 0.01\n",
    "            non_init_word_embeds[current_index] = init_value\n",
    "            non_init_word_embeds_all[current_index_all] = init_value\n",
    "            current_index += 1 #since its a dict, out indexes can be negative probably\n",
    "            current_index_all += 1\n",
    "word_embeddings_en.update(non_init_word_embeds)\n",
    "word_embeddings_all_en.update(non_init_word_embeds_all)\n",
    "current_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.8400e-02,  1.3640e-01,  2.0190e-01, -1.0000e-04,  2.5780e-01,\n",
       "         5.8900e-02, -3.3300e-01,  8.5000e-02,  4.0000e-04,  9.3100e-02,\n",
       "         2.1940e-01,  2.1300e-01,  2.7970e-01,  1.6510e-01,  1.2840e-01,\n",
       "         3.9200e-01,  2.2390e-01, -1.2000e-01,  8.8700e-02,  5.3600e-02,\n",
       "         1.0650e-01,  5.2000e-03,  3.7510e-01, -4.2400e-02,  2.3600e-02,\n",
       "         3.2150e-01,  2.0360e-01,  1.8630e-01, -4.3200e-02,  2.0160e-01,\n",
       "         2.7880e-01,  1.1990e-01, -3.5000e-03, -4.0400e-02,  3.5560e-01,\n",
       "         1.2460e-01, -1.3600e-01, -5.6900e-02,  6.0600e-02,  2.7280e-01,\n",
       "         9.6900e-02,  4.6000e-02, -3.1650e-01,  4.2000e-02, -4.2800e-02,\n",
       "         2.5300e-02,  4.3100e-02,  2.2600e-02, -8.8500e-02, -2.5040e-01,\n",
       "        -7.8100e-02, -2.4600e-02, -1.8000e-02,  3.9900e-02, -1.1280e-01,\n",
       "         2.4850e-01, -1.3300e-02,  8.5500e-02,  9.0000e-03,  1.9590e-01,\n",
       "         2.5100e-02, -1.6200e-02,  2.3690e-01, -7.2000e-03, -1.3850e-01,\n",
       "        -1.9370e-01,  1.1420e-01,  1.6320e-01, -8.0000e-03, -1.4670e-01,\n",
       "        -4.1170e-01, -6.1200e-02, -5.0300e-02, -9.8200e-02,  6.9000e-03,\n",
       "        -2.5300e-02, -2.0300e-02, -1.4540e-01, -1.8810e-01,  2.7210e-01,\n",
       "        -1.5770e-01,  2.3540e-01, -1.9300e-02, -1.1330e-01, -7.2300e-02,\n",
       "         8.6100e-02,  1.7340e-01, -6.4300e-02, -1.0390e-01,  2.4990e-01,\n",
       "        -2.8510e-01,  1.1540e-01, -1.0450e-01,  3.1310e-01,  1.7760e-01,\n",
       "        -1.9210e-01, -1.4140e-01, -6.0700e-02, -1.4310e-01,  1.0810e-01])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings_en[word_indexes_en['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.8400e-02,  1.3640e-01,  2.0190e-01, -1.0000e-04,  2.5780e-01,\n",
       "         5.8900e-02, -3.3300e-01,  8.5000e-02,  4.0000e-04,  9.3100e-02,\n",
       "         2.1940e-01,  2.1300e-01,  2.7970e-01,  1.6510e-01,  1.2840e-01,\n",
       "         3.9200e-01,  2.2390e-01, -1.2000e-01,  8.8700e-02,  5.3600e-02,\n",
       "         1.0650e-01,  5.2000e-03,  3.7510e-01, -4.2400e-02,  2.3600e-02,\n",
       "         3.2150e-01,  2.0360e-01,  1.8630e-01, -4.3200e-02,  2.0160e-01,\n",
       "         2.7880e-01,  1.1990e-01, -3.5000e-03, -4.0400e-02,  3.5560e-01,\n",
       "         1.2460e-01, -1.3600e-01, -5.6900e-02,  6.0600e-02,  2.7280e-01,\n",
       "         9.6900e-02,  4.6000e-02, -3.1650e-01,  4.2000e-02, -4.2800e-02,\n",
       "         2.5300e-02,  4.3100e-02,  2.2600e-02, -8.8500e-02, -2.5040e-01,\n",
       "        -7.8100e-02, -2.4600e-02, -1.8000e-02,  3.9900e-02, -1.1280e-01,\n",
       "         2.4850e-01, -1.3300e-02,  8.5500e-02,  9.0000e-03,  1.9590e-01,\n",
       "         2.5100e-02, -1.6200e-02,  2.3690e-01, -7.2000e-03, -1.3850e-01,\n",
       "        -1.9370e-01,  1.1420e-01,  1.6320e-01, -8.0000e-03, -1.4670e-01,\n",
       "        -4.1170e-01, -6.1200e-02, -5.0300e-02, -9.8200e-02,  6.9000e-03,\n",
       "        -2.5300e-02, -2.0300e-02, -1.4540e-01, -1.8810e-01,  2.7210e-01,\n",
       "        -1.5770e-01,  2.3540e-01, -1.9300e-02, -1.1330e-01, -7.2300e-02,\n",
       "         8.6100e-02,  1.7340e-01, -6.4300e-02, -1.0390e-01,  2.4990e-01,\n",
       "        -2.8510e-01,  1.1540e-01, -1.0450e-01,  3.1310e-01,  1.7760e-01,\n",
       "        -1.9210e-01, -1.4140e-01, -6.0700e-02, -1.4310e-01,  1.0810e-01])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeds_tensor_en = torch.zeros(20326, 100)\n",
    "for i in word_embeddings_en:\n",
    "    word_embeds_tensor_en[i] = word_embeddings_en[i]\n",
    "word_embeds_tensor_en.size()\n",
    "#init_embeds_tensor = torch.stack(tuple([word_embeddings_en[i] for i in word_embeddings_en]))\n",
    "#init_embeds_tensor = torch.tensor(dtype=torch.long)\n",
    "#for i in range(len(word_embeddings_en)):\n",
    "#    init_embeds_tensor.stack\n",
    "#word_embeds_en = nn.Embedding(len(word_embeddings_en), 100)\n",
    "#lookup_tensor = torch.tensor([word_indexes_en[\"the\"]], dtype=torch.long)\n",
    "#word_embeds_en(lookup_tensor)\n",
    "word_embeds_tensor_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency list of training xpos\n",
    "train_xpos_en = {}\n",
    "for i in ud_ewt_train:\n",
    "    for j in i['xpos']:\n",
    "        if j not in train_xpos_en:\n",
    "            train_xpos_en[j] = 1\n",
    "        else:\n",
    "            train_xpos_en[j] += 1\n",
    "train_xpos_en = dict(sorted(train_xpos_en.items(), key=lambda item: item[1], reverse=True))\n",
    "#train_xpos_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency list of training upos\n",
    "train_upos_en = {}\n",
    "for i in ud_ewt_train:\n",
    "    for j in i['upos']:\n",
    "        if upos_map[j] not in train_upos_en:\n",
    "            train_upos_en[upos_map[j]] = 1\n",
    "        else:\n",
    "            train_upos_en[upos_map[j]] += 1\n",
    "train_upos_en = dict(sorted(train_upos_en.items(), key=lambda item: item[1], reverse=True))\n",
    "#train_upos_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency list of training deps\n",
    "train_deps_en = {}\n",
    "for i in ud_ewt_train:\n",
    "    for j in i['deprel']:\n",
    "        if j not in train_deps_en:\n",
    "            train_deps_en[j] = 1\n",
    "        else:\n",
    "            train_deps_en[j] += 1\n",
    "train_deps_en = dict(sorted(train_deps_en.items(), key=lambda item: item[1], reverse=True))\n",
    "#train_deps_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 100])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomly initialize pos and label embeddings (Chen and Manning 3.2) \n",
    "#xpos_tags is full list of arcs in conllu documentation. also has NULL appended\n",
    "xpos_tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '$', ':', ',', '.', \"``\", '’’', '#', '-LRB-', '-RRB-', 'HYPH', 'NFP', 'SYM', 'PUNC', '_', 'NULL']\n",
    "index = 0\n",
    "xpos_indexes = {}\n",
    "for j in set([i for i in train_xpos_en]) | set(xpos_tags): #combine with all xpos found in training for more complete list of xpos\n",
    "    xpos_indexes[j] = index\n",
    "    index += 1\n",
    "xpos_embeddings_en = {}\n",
    "for i in xpos_indexes:\n",
    "    xpos_embeddings_en[xpos_indexes[i]] = (-0.01 - 0.01) * torch.rand(100) + 0.01\n",
    "    \n",
    "xpos_embeds_tensor_en = torch.zeros(len(xpos_indexes), 100)\n",
    "for i in xpos_embeddings_en:\n",
    "    xpos_embeds_tensor_en[i] = xpos_embeddings_en[i]\n",
    "xpos_embeds_tensor_en.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 100])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same as xpos but for arc labels\n",
    "arc_labels = ['nsubj', 'nsubj:pass', 'nsubj:outer', 'obj', 'iobj', 'csubj', 'csubj:pass', 'csubj:outer', 'ccomp', 'xcomp', 'obl', 'obl:npmod', 'obl:tmod', 'advcl', 'advcl:relcl', 'advmod', 'vocative', 'discourse', 'expl', 'aux', 'aux:pass', 'cop', 'mark', 'nummod', 'appos', 'nmod', 'nmod:npmod', 'nmod:tmod', 'nmod:poss', 'acl', 'acl:relcl', 'amod', 'det', 'det:predet', 'compound', 'compound:prt', 'fixed', 'flat', 'flat:foreign', 'goeswith', 'conj', 'cc', 'cc:preconj', 'case', 'list', 'dislocated', 'parataxis', 'orphan', 'reparandum', 'root', 'punct', 'dep', 'NULL']\n",
    "#arc_labels is generic list of arcs. also has NULL appended\n",
    "index = 0\n",
    "arc_indexes = {}\n",
    "for j in set([i for i in train_deps_en]) | set(arc_labels):\n",
    "    arc_indexes[j] = index\n",
    "    index += 1\n",
    "arc_embeddings_en = {}\n",
    "for i in arc_indexes:\n",
    "    arc_embeddings_en[arc_indexes[i]] = (-0.01 - 0.01) * torch.rand(100) + 0.01\n",
    "    \n",
    "arc_embeds_tensor_en = torch.zeros(len(arc_indexes), 100)\n",
    "for i in arc_embeddings_en:\n",
    "    arc_embeds_tensor_en[i] = arc_embeddings_en[i]\n",
    "arc_embeds_tensor_en.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
