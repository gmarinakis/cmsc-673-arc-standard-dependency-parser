{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28f7eebb530>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"universal_dependencies\"\n",
    "ud_config = get_dataset_config_names(name)\n",
    "ud_ewt_train = load_dataset(name, 'en_ewt', split=\"train\")\n",
    "ud_ewt_dev = load_dataset(name, 'en_ewt', split=\"validation\")\n",
    "ud_ewt_test = load_dataset(name, 'en_ewt', split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 'weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000-0035',\n",
       " 'text': 'The situation in Iraq is only going to get better this way.',\n",
       " 'tokens': ['The',\n",
       "  'situation',\n",
       "  'in',\n",
       "  'Iraq',\n",
       "  'is',\n",
       "  'only',\n",
       "  'going',\n",
       "  'to',\n",
       "  'get',\n",
       "  'better',\n",
       "  'this',\n",
       "  'way',\n",
       "  '.'],\n",
       " 'lemmas': ['the',\n",
       "  'situation',\n",
       "  'in',\n",
       "  'Iraq',\n",
       "  'be',\n",
       "  'only',\n",
       "  'go',\n",
       "  'to',\n",
       "  'get',\n",
       "  'better',\n",
       "  'this',\n",
       "  'way',\n",
       "  '.'],\n",
       " 'upos': [8, 0, 2, 10, 17, 14, 16, 7, 16, 6, 8, 0, 1],\n",
       " 'xpos': ['DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'VBZ',\n",
       "  'RB',\n",
       "  'VBG',\n",
       "  'TO',\n",
       "  'VB',\n",
       "  'JJR',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  '.'],\n",
       " 'feats': [\"{'Definite': 'Def', 'PronType': 'Art'}\",\n",
       "  \"{'Number': 'Sing'}\",\n",
       "  'None',\n",
       "  \"{'Number': 'Sing'}\",\n",
       "  \"{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\",\n",
       "  'None',\n",
       "  \"{'Tense': 'Pres', 'VerbForm': 'Part'}\",\n",
       "  'None',\n",
       "  \"{'VerbForm': 'Inf'}\",\n",
       "  \"{'Degree': 'Cmp'}\",\n",
       "  \"{'Number': 'Sing', 'PronType': 'Dem'}\",\n",
       "  \"{'Number': 'Sing'}\",\n",
       "  'None'],\n",
       " 'head': ['2', '7', '4', '2', '7', '7', '0', '9', '7', '9', '12', '10', '7'],\n",
       " 'deprel': ['det',\n",
       "  'nsubj',\n",
       "  'case',\n",
       "  'nmod',\n",
       "  'aux',\n",
       "  'advmod',\n",
       "  'root',\n",
       "  'mark',\n",
       "  'xcomp',\n",
       "  'xcomp',\n",
       "  'det',\n",
       "  'obj',\n",
       "  'punct'],\n",
       " 'deps': [\"[('det', 2)]\",\n",
       "  \"[('nsubj', 7), ('nsubj:xsubj', 9), ('nsubj:xsubj', 10)]\",\n",
       "  \"[('case', 4)]\",\n",
       "  \"[('nmod:in', 2)]\",\n",
       "  \"[('aux', 7)]\",\n",
       "  \"[('advmod', 7)]\",\n",
       "  \"[('root', 0)]\",\n",
       "  \"[('mark', 9)]\",\n",
       "  \"[('xcomp', 7)]\",\n",
       "  \"[('xcomp', 9)]\",\n",
       "  \"[('det', 12)]\",\n",
       "  \"[('obj', 10)]\",\n",
       "  \"[('punct', 7)]\"],\n",
       " 'misc': ['None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  'None',\n",
       "  \"{'SpaceAfter': 'No'}\",\n",
       "  'None']}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ud_ewt_train[34] is good default\n",
    "ud_ewt_train[34]['tokens']\n",
    "ud_ewt_train[34]['upos'] #Part of speech tag as an integer index of [\"NOUN\",\"PUNCT\",\"ADP\",\"NUM\",\"SYM\",\"SCONJ\",\"ADJ\",\"PART\",\"DET\",\"CCONJ\",\"PROPN\",\"PRON\",\"X\",\"_\",\"ADV\",\"INTJ\",\"VERB\",\"AUX\"]\n",
    "upos_map = [\"NOUN\",\"PUNCT\",\"ADP\",\"NUM\",\"SYM\",\"SCONJ\",\"ADJ\",\"PART\",\"DET\",\"CCONJ\",\"PROPN\",\"PRON\",\"X\",\"_\",\"ADV\",\"INTJ\",\"VERB\",\"AUX\"]\n",
    "ud_ewt_train[34]['xpos'] #Other POS tag, might be more accurate?\n",
    "ud_ewt_train[34]['deprel'] #arc labels by themselves. https://universaldependencies.org/docs/u/dep/index.html\n",
    "ud_ewt_train[34]['deps'] #arc labels and the index they interact with. NOTE: index starts at one, ROOT is assumed\n",
    "ud_ewt_train[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentence from exmaple in chen and manning\n",
    "test_sent = {'text': 'He has really good control.',\n",
    "             'tokens': ['He', 'has', 'good', 'control', '.'],\n",
    "             'upos': [11, 16, 6, 0, 1], #probably need to use xpos,\n",
    "             'xpos': ['PRP', 'VBZ', 'JJ', 'NN', '.'],\n",
    "             'deprel': ['nsubj', 'root', 'amod', 'obj', 'punct'], #apparently dobj doesnt exist anymore, so the example in Chen and Manning isnt accurate. replaced with obj instead\n",
    "             'deps': [\"[('nsubj', 2)]\", \"[('root', 0)]\", \"[('amod', 4)]\", \"[('obj', 2)]\", \"[('punct', 2)]\"] #these are strings for some reason\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEFT-ARC(l): adds an arc s1 → s2 with label l and removes s2 from the stack. Precondition: |s| ≥ 2. from Chen and Manning\n",
    "def left_arc(stack, buffer, arcs, dep, print_output=True):\n",
    "    if len(stack) < 2:\n",
    "        #print(\"[@] LEFT-ARC called incorrectly, check stack size\")\n",
    "        return False\n",
    "    elif stack[-2] == \"[ROOT]\":\n",
    "        #print(\"[@] LEFT-ARC called incorrectly, tried to add depepndency to ROOT\")\n",
    "        return False\n",
    "    else:\n",
    "        arcs.append((dep, (stack[-1],stack.pop(-2))))\n",
    "    if print_output:\n",
    "        print(\"Stack: \", stack, end=\" | \")\n",
    "        print(\"Buffer: \", buffer, end=\" | \")\n",
    "        print(\"Arcs:\", arcs)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RIGHT-ARC(l): adds an arc s2 → s1 with label l and removes s1 from the stack. Precondition: |s| ≥ 2. from Chen and Manning\n",
    "def right_arc(stack, buffer, arcs, dep, print_output=True):\n",
    "    if len(stack) < 2:\n",
    "        #print(\"[@] RIGHT-ARC called incorrectly, check stack size\")\n",
    "        return False\n",
    "    else:\n",
    "        arcs.append((dep, (stack[-2],stack.pop(-1))))\n",
    "    if print_output:\n",
    "        print(\"Stack: \", stack, end=\" | \")\n",
    "        print(\"Buffer: \", buffer, end=\" | \")\n",
    "        print(\"Arcs:\", arcs)\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHIFT: moves b1 from the buffer to the stack. Precondition: |b| ≥ 1. from Chen and Manning\n",
    "def shift(stack, buffer, arcs, print_output=True):\n",
    "    if len(buffer) < 1:\n",
    "        #print(\"[@] SHIFT called incorrectly, check buffer size\")\n",
    "        return False\n",
    "    else:\n",
    "        stack.append(buffer.pop(0))\n",
    "    if print_output:\n",
    "        print(\"Stack: \", stack, end=\" | \")\n",
    "        print(\"Buffer: \", buffer, end=\" | \")\n",
    "        print(\"Arcs:\", arcs)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack:  ['[ROOT]', 'He'] | Buffer:  ['has', 'good', 'control', '.'] | Arcs: []\n",
      "Stack:  ['[ROOT]', 'He', 'has'] | Buffer:  ['good', 'control', '.'] | Arcs: []\n",
      "Stack:  ['[ROOT]', 'has'] | Buffer:  ['good', 'control', '.'] | Arcs: [('nsubj', ('has', 'He'))]\n",
      "Stack:  ['[ROOT]', 'has', 'good'] | Buffer:  ['control', '.'] | Arcs: [('nsubj', ('has', 'He'))]\n",
      "Stack:  ['[ROOT]', 'has', 'good', 'control'] | Buffer:  ['.'] | Arcs: [('nsubj', ('has', 'He'))]\n",
      "Stack:  ['[ROOT]', 'has', 'control'] | Buffer:  ['.'] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good'))]\n",
      "Stack:  ['[ROOT]', 'has'] | Buffer:  ['.'] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good')), ('obj', ('has', 'control'))]\n",
      "Stack:  ['[ROOT]', 'has', '.'] | Buffer:  [] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good')), ('obj', ('has', 'control'))]\n",
      "Stack:  ['[ROOT]', 'has'] | Buffer:  [] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good')), ('obj', ('has', 'control')), ('punct', ('has', '.'))]\n",
      "Stack:  ['[ROOT]'] | Buffer:  [] | Arcs: [('nsubj', ('has', 'He')), ('amod', ('control', 'good')), ('obj', ('has', 'control')), ('punct', ('has', '.')), ('root', ('[ROOT]', 'has'))]\n"
     ]
    }
   ],
   "source": [
    "#Goal is to predict the correct transitions at each step aka predict the deps and timing of deps correctly\n",
    "#Sample of \"correct\" transitions for \"He has good control.\"\n",
    "test_stack = [\"[ROOT]\"]\n",
    "test_buffer = copy.deepcopy(test_sent['tokens'])\n",
    "# buffer = [(test_sent['tokens'][i], test_sent['tokens'][i]) for i in range(len(test_sent['tokens']))] #include xpos in buffer in the future\n",
    "test_arcs = []\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "left_arc(test_stack, test_buffer, test_arcs, 'nsubj')\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "left_arc(test_stack, test_buffer, test_arcs, 'amod')\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'obj')\n",
    "shift(test_stack, test_buffer, test_arcs)\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'punct')\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nsubj', ('has', 'He')),\n",
       " ('amod', ('control', 'good')),\n",
       " ('obj', ('has', 'control')),\n",
       " ('punct', ('has', '.')),\n",
       " ('root', ('[ROOT]', 'has'))]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nsubj', ('has', 'He')),\n",
       " ('root', ('[ROOT]', 'has')),\n",
       " ('amod', ('control', 'good')),\n",
       " ('obj', ('has', 'control')),\n",
       " ('punct', ('has', '.'))]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare to arcs in UD format\n",
    "# [\"[('nsubj', 2)]\", \"[('root', 0)]\", \"[('amod', 4)]\", \"[('dobj', 2)]\", \"[('punct', 2)]\"]\n",
    "valid_arcs = []\n",
    "for i in range(len(test_sent['deps'])):\n",
    "    curr_dep = ast.literal_eval(test_sent['deps'][i])[0]\n",
    "    valid_arcs.append((curr_dep[0],((([\"[ROOT]\"] + test_sent['tokens'])[curr_dep[1]]), test_sent['tokens'][i])))\n",
    "valid_arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('det', (('situation', 'NN'), ('The', 'DT'))),\n",
       " ('nsubj', (('going', 'VBG'), ('situation', 'NN'))),\n",
       " ('case', (('Iraq', 'NNP'), ('in', 'IN'))),\n",
       " ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))),\n",
       " ('aux', (('going', 'VBG'), ('is', 'VBZ'))),\n",
       " ('advmod', (('going', 'VBG'), ('only', 'RB'))),\n",
       " ('root', (('[ROOT]', 'ROOT'), ('going', 'VBG'))),\n",
       " ('mark', (('get', 'VB'), ('to', 'TO'))),\n",
       " ('xcomp', (('going', 'VBG'), ('get', 'VB'))),\n",
       " ('xcomp', (('get', 'VB'), ('better', 'JJR'))),\n",
       " ('det', (('way', 'NN'), ('this', 'DT'))),\n",
       " ('obj', (('better', 'JJR'), ('way', 'NN'))),\n",
       " ('punct', (('going', 'VBG'), ('.', '.')))]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent2 = ud_ewt_train[34]\n",
    "valid_arcs2 = []\n",
    "for i in range(len(test_sent2['deps'])):\n",
    "    curr_dep = ast.literal_eval(test_sent2['deps'][i])[0]\n",
    "    valid_arcs2.append((test_sent2['deprel'][i],(((([\"[ROOT]\"] + test_sent2['tokens'])[curr_dep[1]]), ([\"ROOT\"] + test_sent2['xpos'])[curr_dep[1]]), (test_sent2['tokens'][i], test_sent2['xpos'][i])))) #use deprel for the arc name. ignoring enhanced depependencies for now\n",
    "valid_arcs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([i in valid_arcs for i in test_arcs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_oracle(stack, buffer, arcs, unlabeled_arcs, labeled_arcs, transitions):\n",
    "    if len(stack) >= 2:\n",
    "        # print([j[0] for j in unlabeled_arcs])\n",
    "        if (stack[-1],stack[-2]) in unlabeled_arcs:\n",
    "            arc_label = labeled_arcs.pop(unlabeled_arcs.index((stack[-1],stack[-2])))[0]\n",
    "            transitions.append(\"left-arc \" + arc_label)\n",
    "            unlabeled_arcs.remove((stack[-1],stack[-2]))\n",
    "            left_arc(stack, buffer, arcs, arc_label)\n",
    "        elif (stack[-2],stack[-1]) in unlabeled_arcs and stack[-1] not in [j[0] for j in unlabeled_arcs]: #all of the dependents of the word at the top of the stack must already be assign before right arc,\n",
    "            arc_label = labeled_arcs.pop(unlabeled_arcs.index((stack[-2],stack[-1])))[0]\n",
    "            transitions.append(\"right-arc \" + arc_label)\n",
    "            unlabeled_arcs.remove((stack[-2],stack[-1]))\n",
    "            right_arc(stack, buffer, arcs, arc_label)\n",
    "    if len(transitions) <= i: #if neither arc transition has been done do shift\n",
    "        transitions.append(\"shift\")\n",
    "        shift(stack, buffer, arcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To generate training data, need to reverse the valid arcs into operations which produce them\n",
    "#The \"training oracle\" as described in slp3 which determines which transition to do. return a list of transitions\n",
    "#sentence is UD format sentence\n",
    "#Stack/Buffer format: List of tuple with arc label and tuple of (word, POS) tuples in parent, child order\n",
    "#[ ( arc-label, ( (parent-word, parent-POS), (child-word, child-POS) ) ), ... ]\n",
    "def training_sent(sentence):\n",
    "    stack = [(\"[ROOT]\", \"ROOT\")]\n",
    "    # buffer = copy.deepcopy(sentence['tokens'])\n",
    "    buffer = [(sentence['tokens'][i], sentence['xpos'][i]) for i in range(len(sentence['tokens']))]\n",
    "    arcs = []\n",
    "    transitions = [] #what we return\n",
    "    labeled_arcs = []\n",
    "    for i in range(len(sentence['deps'])):\n",
    "        curr_dep = ast.literal_eval(sentence['deps'][i])[0]\n",
    "        labeled_arcs.append((sentence['deprel'][i],(((([\"[ROOT]\"] + sentence['tokens'])[curr_dep[1]]), ([\"ROOT\"] + sentence['xpos'])[curr_dep[1]]), (sentence['tokens'][i], sentence['xpos'][i]))))\n",
    "    labeled_arcs_copy = copy.deepcopy(labeled_arcs)\n",
    "    unlabeled_arcs = [i[1] for i in labeled_arcs]\n",
    "    #print(labeled_arcs_copy)\n",
    "    #print(unlabeled_arcs)\n",
    "    for i in range(2*len(sentence['tokens'])): #2N transitions\n",
    "        training_oracle(stack, buffer, arcs, unlabeled_arcs, labeled_arcs, transitions)\n",
    "    print(\"All generated arcs are in original deps list:\", all([i in labeled_arcs_copy for i in arcs]))\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('root', (('[ROOT]', 'ROOT'), ('has', 'VBZ'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('obj', (('has', 'VBZ'), ('control', 'NN'))), ('punct', (('has', 'VBZ'), ('.', '.')))]\n",
      "[(('has', 'VBZ'), ('He', 'PRP')), (('[ROOT]', 'ROOT'), ('has', 'VBZ')), (('control', 'NN'), ('good', 'JJ')), (('has', 'VBZ'), ('control', 'NN')), (('has', 'VBZ'), ('.', '.'))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('He', 'PRP')] | Buffer:  [('has', 'VBZ'), ('good', 'JJ'), ('control', 'NN'), ('.', '.')] | Arcs: []\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('He', 'PRP'), ('has', 'VBZ')] | Buffer:  [('good', 'JJ'), ('control', 'NN'), ('.', '.')] | Arcs: []\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ')] | Buffer:  [('good', 'JJ'), ('control', 'NN'), ('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('good', 'JJ')] | Buffer:  [('control', 'NN'), ('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('good', 'JJ'), ('control', 'NN')] | Buffer:  [('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('control', 'NN')] | Buffer:  [('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ')] | Buffer:  [('.', '.')] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('obj', (('has', 'VBZ'), ('control', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('.', '.')] | Buffer:  [] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('obj', (('has', 'VBZ'), ('control', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('has', 'VBZ')] | Buffer:  [] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('obj', (('has', 'VBZ'), ('control', 'NN'))), ('punct', (('has', 'VBZ'), ('.', '.')))]\n",
      "Stack:  [('[ROOT]', 'ROOT')] | Buffer:  [] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('obj', (('has', 'VBZ'), ('control', 'NN'))), ('punct', (('has', 'VBZ'), ('.', '.'))), ('root', (('[ROOT]', 'ROOT'), ('has', 'VBZ')))]\n",
      "All generated arcs are in original deps list: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['shift',\n",
       " 'shift',\n",
       " 'left-arc nsubj',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc amod',\n",
       " 'right-arc obj',\n",
       " 'shift',\n",
       " 'right-arc punct',\n",
       " 'right-arc root']"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sent(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack:  [('[ROOT]', 'ROOT'), ('The', 'DT')] | Buffer:  [('situation', 'NN'), ('in', 'IN'), ('Iraq', 'NNP'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: []\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('The', 'DT'), ('situation', 'NN')] | Buffer:  [('in', 'IN'), ('Iraq', 'NNP'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: []\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN')] | Buffer:  [('in', 'IN'), ('Iraq', 'NNP'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('in', 'IN')] | Buffer:  [('Iraq', 'NNP'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('in', 'IN'), ('Iraq', 'NNP')] | Buffer:  [('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('Iraq', 'NNP')] | Buffer:  [('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN')] | Buffer:  [('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('is', 'VBZ')] | Buffer:  [('only', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('is', 'VBZ'), ('only', 'RB')] | Buffer:  [('going', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('is', 'VBZ'), ('only', 'RB'), ('going', 'VBG')] | Buffer:  [('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('is', 'VBZ'), ('going', 'VBG')] | Buffer:  [('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('situation', 'NN'), ('going', 'VBG')] | Buffer:  [('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG')] | Buffer:  [('to', 'TO'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('to', 'TO')] | Buffer:  [('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('to', 'TO'), ('get', 'VB')] | Buffer:  [('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB')] | Buffer:  [('better', 'JJR'), ('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR')] | Buffer:  [('this', 'DT'), ('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT')] | Buffer:  [('way', 'NN'), ('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR'), ('this', 'DT'), ('way', 'NN')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR'), ('way', 'NN')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB'), ('better', 'JJR')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('get', 'VB')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG')] | Buffer:  [('.', '.')] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG'), ('.', '.')] | Buffer:  [] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB')))]\n",
      "Stack:  [('[ROOT]', 'ROOT'), ('going', 'VBG')] | Buffer:  [] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB'))), ('punct', (('going', 'VBG'), ('.', '.')))]\n",
      "Stack:  [('[ROOT]', 'ROOT')] | Buffer:  [] | Arcs: [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB'))), ('punct', (('going', 'VBG'), ('.', '.'))), ('root', (('[ROOT]', 'ROOT'), ('going', 'VBG')))]\n",
      "All generated arcs are in original deps list: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['shift',\n",
       " 'shift',\n",
       " 'left-arc det',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc case',\n",
       " 'right-arc nmod',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc advmod',\n",
       " 'left-arc aux',\n",
       " 'left-arc nsubj',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc mark',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'left-arc det',\n",
       " 'right-arc obj',\n",
       " 'right-arc xcomp',\n",
       " 'right-arc xcomp',\n",
       " 'shift',\n",
       " 'right-arc punct',\n",
       " 'right-arc root']"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sent(ud_ewt_train[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Featureization. We use sets of elements Sw St and Sl as described in 3.1 of Chen and Manning which can be combined to create features\n",
    "def featurize_configuration(stack, buffer, arcs):\n",
    "    S_w = {'s1' : 'NULL', 's2': 'NULL', 's3': 'NULL', 'b1': 'NULL', 'b2': 'NULL', 'b3': 'NULL',\n",
    "           'lc1s1' : 'NULL', 'lc2s1' : 'NULL', 'lc1s2' : 'NULL', 'lc2s2' : 'NULL', 'rc1s1' : 'NULL', 'rc2s1' : 'NULL', 'rc1s2' : 'NULL', 'rc2s2' : 'NULL', #lc1s1 is leftmost child of s1, lc2s1 is second leftmost\n",
    "           'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'} #lc1lc1 is leftmost child of leftmost children\n",
    "    S_t = {'s1' : 'NULL', 's2': 'NULL', 's3': 'NULL', 'b1': 'NULL', 'b2': 'NULL', 'b3': 'NULL',\n",
    "           'lc1s1' : 'NULL', 'lc2s1' : 'NULL', 'lc1s2' : 'NULL', 'lc2s2' : 'NULL', 'rc1s1' : 'NULL', 'rc2s1' : 'NULL', 'rc1s2' : 'NULL', 'rc2s2' : 'NULL',\n",
    "           'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'}\n",
    "    S_l = {'lc1s1' : 'NULL', 'lc2s1' : 'NULL', 'lc1s2' : 'NULL', 'lc2s2' : 'NULL', 'rc1s1' : 'NULL', 'rc2s1' : 'NULL', 'rc1s2' : 'NULL', 'rc2s2' : 'NULL',\n",
    "           'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'}\n",
    "    \n",
    "    #s1 =============================================================\n",
    "    if len(stack) >= 1:\n",
    "       S_w['s1'] = stack[-1][0]\n",
    "       S_t['s1'] = stack[-1][1]\n",
    "       parent_list = [j[0] for j in [i[1] for i in arcs]]\n",
    "       arcs_copy = copy.deepcopy(arcs)\n",
    "       \n",
    "       #first leftmost child\n",
    "      #  print(parent_list)\n",
    "       if stack[-1] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = parent_list.index(stack[-1])\n",
    "           lc1_arc = arcs_copy[idx]\n",
    "          #  print(lc1_arc)\n",
    "           S_w['lc1s1'] = lc1_arc[1][1][0]\n",
    "           S_t['lc1s1'] = lc1_arc[1][1][1]\n",
    "           S_l['lc1s1'] = lc1_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "           #leftmost child of leftmost child\n",
    "           if lc1_arc[1][1] in parent_list:\n",
    "              idx = parent_list.index(lc1_arc[1][1])\n",
    "              lc1lc1_arc = arcs_copy[idx]\n",
    "              S_w['lc1lc1s1'] = lc1lc1_arc[1][1][0]\n",
    "              S_t['lc1lc1s1'] = lc1lc1_arc[1][1][1]\n",
    "              S_l['lc1lc1s1'] = lc1lc1_arc[0]\n",
    "              parent_list.pop(idx)\n",
    "              arcs_copy.pop(idx)\n",
    "      #  print(S_w)\n",
    "       #first rightmost child       \n",
    "       if stack[-1] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = len(parent_list) - parent_list[-1::-1].index(stack[-1]) - 1 #index of last occurence list slicing nonsense\n",
    "           rc1_arc = arcs_copy[idx] \n",
    "           S_w['rc1s1'] = rc1_arc[1][1][0]\n",
    "           S_t['rc1s1'] = rc1_arc[1][1][1]\n",
    "           S_l['rc1s1'] = rc1_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "          #  print(parent_list)\n",
    "           #rightmost child of rightmost child\n",
    "           if rc1_arc[1][1] in parent_list:\n",
    "              idx = len(parent_list) - parent_list[-1::-1].index(rc1_arc[1][1]) - 1\n",
    "              rc1rc1_arc = arcs_copy[idx]\n",
    "              S_w['rc1rc1s1'] = rc1rc1_arc[1][1][0]\n",
    "              S_t['rc1rc1s1'] = rc1rc1_arc[1][1][1]\n",
    "              S_l['rc1rc1s1'] = rc1rc1_arc[0]\n",
    "              parent_list.pop(idx)\n",
    "              arcs_copy.pop(idx)\n",
    "       \n",
    "       #second leftmost child\n",
    "       if stack[-1] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = parent_list.index(stack[-1])\n",
    "           lc2_arc = arcs_copy[idx]\n",
    "           S_w['lc2s1'] = lc2_arc[1][1][0]\n",
    "           S_t['lc2s1'] = lc2_arc[1][1][1]\n",
    "           S_l['lc2s1'] = lc2_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "       \n",
    "       #second rightmost child       \n",
    "       if stack[-1] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = len(parent_list) - parent_list[-1::-1].index(stack[-1]) - 1 #index of last occurence list slicing nonsense\n",
    "           rc2_arc = arcs_copy[idx] \n",
    "           S_w['rc2s1'] = rc2_arc[1][1][0]\n",
    "           S_t['rc2s1'] = rc2_arc[1][1][1]\n",
    "           S_l['rc2s1'] = rc2_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "       \n",
    "       #s2 =================================================================\n",
    "       if len(stack) >= 2:       \n",
    "         S_w['s2'] = stack[-2][0]\n",
    "         S_t['s2'] = stack[-2][1]\n",
    "           \n",
    "         #first leftmost child\n",
    "         if stack[-2] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = parent_list.index(stack[-2])\n",
    "           lc1_arc = arcs_copy[idx]\n",
    "       #     print(lc1_arc)\n",
    "           S_w['lc1s2'] = lc1_arc[1][1][0]\n",
    "           S_t['lc1s2'] = lc1_arc[1][1][1]\n",
    "           S_l['lc1s2'] = lc1_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "           #leftmost child of leftmost child\n",
    "           if lc1_arc[1][1] in parent_list:\n",
    "              idx = parent_list.index(lc1_arc[1][1])\n",
    "              lc1lc1_arc = arcs_copy[idx]\n",
    "              S_w['lc1lc1s2'] = lc1lc1_arc[1][1][0]\n",
    "              S_t['lc1lc1s2'] = lc1lc1_arc[1][1][1]\n",
    "              S_l['lc1lc1s2'] = lc1lc1_arc[0]\n",
    "              parent_list.pop(idx)\n",
    "              arcs_copy.pop(idx)\n",
    "       \n",
    "         #first rightmost child       \n",
    "         if stack[-2] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = len(parent_list) - parent_list[-1::-1].index(stack[-2]) - 1 #index of last occurence list slicing nonsense\n",
    "           rc1_arc = arcs_copy[idx] \n",
    "           S_w['rc1s2'] = rc1_arc[1][1][0]\n",
    "           S_t['rc1s2'] = rc1_arc[1][1][1]\n",
    "           S_l['rc1s2'] = rc1_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "           #rightmost child of rightmost child\n",
    "           if rc1_arc[1][1] in parent_list:\n",
    "              idx = len(parent_list) - parent_list[-1::-1].index(rc1_arc[1][1]) - 1\n",
    "              rc1rc1_arc = arcs_copy[idx]\n",
    "              S_w['rc1rc1s2'] = rc1rc1_arc[1][1][0]\n",
    "              S_t['rc1rc1s2'] = rc1rc1_arc[1][1][1]\n",
    "              S_l['rc1rc1s2'] = rc1rc1_arc[0]\n",
    "              parent_list.pop(idx)\n",
    "              arcs_copy.pop(idx)\n",
    "       \n",
    "         #second leftmost child\n",
    "         if stack[-2] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = parent_list.index(stack[-2])\n",
    "           lc2_arc = arcs_copy[idx]\n",
    "           S_w['lc2s2'] = lc2_arc[1][1][0]\n",
    "           S_t['lc2s2'] = lc2_arc[1][1][1]\n",
    "           S_l['lc2s2'] = lc2_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "       \n",
    "         #second rightmost child       \n",
    "         if stack[-2] in parent_list:\n",
    "           #arcs are added sequentially, so the lowest index child is leftmost/highest index child is rightmost\n",
    "           idx = len(parent_list) - parent_list[-1::-1].index(stack[-2]) - 1 #index of last occurence list slicing nonsense\n",
    "           rc2_arc = arcs_copy[idx] \n",
    "           S_w['rc2s2'] = rc2_arc[1][1][0]\n",
    "           S_t['rc2s2'] = rc2_arc[1][1][1]\n",
    "           S_l['rc2s2'] = rc2_arc[0]\n",
    "           parent_list.pop(idx)\n",
    "           arcs_copy.pop(idx)\n",
    "           \n",
    "         if len(stack) >= 3:\n",
    "           S_w['s3'] = stack[-3][0]\n",
    "           S_t['s3'] = stack[-3][1]\n",
    "\n",
    "    if len(buffer) >= 1:\n",
    "       S_w['b1'] = buffer[-1][0]\n",
    "       S_t['b1'] = buffer[-1][1]\n",
    "       if len(buffer) >= 2:       \n",
    "           S_w['b2'] = buffer[-2][0]\n",
    "           S_t['b2'] = buffer[-2][1]\n",
    "           if len(buffer) >= 3:\n",
    "              S_w['b3'] = buffer[-3][0]\n",
    "              S_t['b3'] = buffer[-3][1]\n",
    "    \n",
    "    return S_w, S_t, S_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack:  [('[ROOT]', 'NULL'), ('has', 'VBZ')] | Buffer:  [] | Arcs: [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('amod', (('control', 'NN'), ('good', 'JJ'))), ('obj', (('has', 'VBZ'), ('control', 'NN'))), ('punct', (('has', 'VBZ'), ('.', '.')))]\n",
      "({'s1': 'has', 's2': '[ROOT]', 's3': 'NULL', 'b1': 'NULL', 'b2': 'NULL', 'b3': 'NULL', 'lc1s1': 'He', 'lc2s1': 'control', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': '.', 'rc2s1': 'NULL', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'}, {'s1': 'VBZ', 's2': 'NULL', 's3': 'NULL', 'b1': 'NULL', 'b2': 'NULL', 'b3': 'NULL', 'lc1s1': 'PRP', 'lc2s1': 'NN', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': '.', 'rc2s1': 'NULL', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'}, {'lc1s1': 'nsubj', 'lc2s1': 'obj', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': 'punct', 'rc2s1': 'NULL', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'NULL', 'rc1rc1s2': 'NULL'})\n"
     ]
    }
   ],
   "source": [
    "#Testing the feature list\n",
    "test_stack = [(\"[ROOT]\", \"NULL\")]\n",
    "test_buffer = [(test_sent['tokens'][i], test_sent['xpos'][i]) for i in range(len(test_sent['tokens']))]\n",
    "test_arcs = []\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "left_arc(test_stack, test_buffer, test_arcs, 'nsubj', False)\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "left_arc(test_stack, test_buffer, test_arcs, 'amod', False)\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'obj', False)\n",
    "shift(test_stack, test_buffer, test_arcs, False)\n",
    "right_arc(test_stack, test_buffer, test_arcs, 'punct')\n",
    "test_S = featurize_configuration(test_stack, test_buffer, test_arcs)\n",
    "print(test_S)\n",
    "# right_arc(test_stack, test_buffer, test_arcs, 'root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'s1': 'going', 's2': '[ROOT]', 's3': 'NULL', 'b1': '.', 'b2': 'NULL', 'b3': 'NULL', 'lc1s1': 'only', 'lc2s1': 'is', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': 'get', 'rc2s1': 'situation', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'better', 'rc1rc1s2': 'NULL'}, {'s1': 'VBG', 's2': 'ROOT', 's3': 'NULL', 'b1': '.', 'b2': 'NULL', 'b3': 'NULL', 'lc1s1': 'RB', 'lc2s1': 'VBZ', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': 'VB', 'rc2s1': 'NN', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'JJR', 'rc1rc1s2': 'NULL'}, {'lc1s1': 'advmod', 'lc2s1': 'aux', 'lc1s2': 'NULL', 'lc2s2': 'NULL', 'rc1s1': 'xcomp', 'rc2s1': 'nsubj', 'rc1s2': 'NULL', 'rc2s2': 'NULL', 'lc1lc1s1': 'NULL', 'lc1lc1s2': 'NULL', 'rc1rc1s1': 'xcomp', 'rc1rc1s2': 'NULL'})\n"
     ]
    }
   ],
   "source": [
    "test_stack3 = [('[ROOT]', 'ROOT'), ('going', 'VBG')] \n",
    "test_buffer3 = [('.', '.')]\n",
    "test_arcs3 = [('det', (('situation', 'NN'), ('The', 'DT'))), ('case', (('Iraq', 'NNP'), ('in', 'IN'))), ('nmod', (('situation', 'NN'), ('Iraq', 'NNP'))), ('advmod', (('going', 'VBG'), ('only', 'RB'))), ('aux', (('going', 'VBG'), ('is', 'VBZ'))), ('nsubj', (('going', 'VBG'), ('situation', 'NN'))), ('mark', (('get', 'VB'), ('to', 'TO'))), ('det', (('way', 'NN'), ('this', 'DT'))), ('obj', (('better', 'JJR'), ('way', 'NN'))), ('xcomp', (('get', 'VB'), ('better', 'JJR'))), ('xcomp', (('going', 'VBG'), ('get', 'VB')))]\n",
    "print(featurize_configuration(test_stack3, test_buffer3, test_arcs3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency list of training vocab\n",
    "train_vocab_en = {}\n",
    "for i in ud_ewt_train:\n",
    "    for j in i['tokens']:\n",
    "        if j not in train_vocab_en:\n",
    "            train_vocab_en[j] = 1\n",
    "        else:\n",
    "            train_vocab_en[j] += 1\n",
    "train_vocab_en = dict(sorted(train_vocab_en.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13654"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use pretrained word embeddings from wikipedia2vec. I recognize that the vocabulary is needlessly large, but why not\n",
    "file = open(\"enwiki_20180420_win10_100d.txt\", \"r\", encoding='utf-8')\n",
    "#nn.embedding uses indexes to map words, so there will be a dictionary of {word:index} and a dictionary of {index:embeddings}\n",
    "#the _all dicts contain all of the embeddings as initialized, useful for referencing only the prtrained embedding values. tokens not in vocab won't be modified anyways\n",
    "word_indexes_en = {}\n",
    "word_indexes_all_en = {}\n",
    "word_embeddings_en = {} \n",
    "word_embeddings_all_en = {}\n",
    "#word_embeddings_en = nn.Embedding(num_embeddings=4530030, embedding_dim=100)\n",
    "skip_first = True\n",
    "has_next=True #use a while with boolean so I can continue through UnicodeDecodeErros and skip weird characters which arent in the vocab anyways\n",
    "current_index_all = 0\n",
    "current_index = 0\n",
    "while has_next:\n",
    "    if not skip_first:\n",
    "        try:\n",
    "            line = file.readline()\n",
    "            if line != \"\":\n",
    "                if not (\"ENTITY/\" in line and \"_\" in line): #We dont care about entities // We can try adding single token entities since most of them are proper nouns, can differnciate by case\n",
    "                    current_line = line.split()\n",
    "                    #print(current_line)\n",
    "                    if current_line != []:\n",
    "                        if \"ENTITY/\" in current_line[0]:\n",
    "                            current_token = current_line[0][current_line[0].index(\"/\")+1:]\n",
    "                        else:\n",
    "                            current_token = current_line[0]\n",
    "                        #print(current_token)\n",
    "                        current_embeds = []\n",
    "                        if len(current_line) == 101: #some words in the pretrained embeddings have a duplication with a space in them (ex. 8_Flora vs 8 Flora, Channel_4 vs Channel 4). Ignore the ones with a space\n",
    "                            for i in current_line[1:]:\n",
    "                                    current_embeds.append(float(i))\n",
    "                            word_indexes_all_en[current_token] = current_index_all\n",
    "                            word_embeddings_all_en[current_index_all] = torch.FloatTensor(current_embeds)\n",
    "                            if current_token in train_vocab_en:\n",
    "                                word_indexes_en[current_token] = current_index\n",
    "                                word_embeddings_en[current_index] = torch.FloatTensor(current_embeds)\n",
    "                                current_index += 1\n",
    "                            current_index_all += 1\n",
    "            else:\n",
    "                has_next=False\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    else: #first line is skipped\n",
    "        file.readline()\n",
    "        skip_first = False\n",
    "file.close()\n",
    "current_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11445 friendlier 11445\n",
      "11446 Confirmation 11446\n",
      "11447 Butcher 11448\n",
      "11448 chairpersons 11449\n",
      "\n",
      "225 2015 11447\n"
     ]
    }
   ],
   "source": [
    "#why does this happen???\n",
    "count = 0\n",
    "for i in word_indexes_en:\n",
    "    if count >= 11445 and count <= 11448:\n",
    "        print(count, i, word_indexes_en[i])\n",
    "    count += 1\n",
    "print(\"\")\n",
    "magic_11447 = '2015'\n",
    "print([i for i in word_indexes_en].index(magic_11447), magic_11447, word_indexes_en[magic_11447])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20327"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomly initialize non intialized embeddings\n",
    "#uses current index and current index all from previous block\n",
    "non_init_word_indexes = {}\n",
    "non_init_word_indexes_all = {}\n",
    "non_init_word_embeds = {}\n",
    "non_init_word_embeds_all = {}\n",
    "train_vocab_en[\"NULL\"] = 0 #add NULL token\n",
    "train_vocab_en[\"[ROOT]\"] = 0 #add root token\n",
    "for i in train_vocab_en:\n",
    "    if i not in word_indexes_en:\n",
    "        if i not in non_init_word_indexes:\n",
    "            non_init_word_indexes[i] = current_index\n",
    "            non_init_word_indexes_all[i] = current_index_all\n",
    "            init_value = (-0.01 - 0.01) * torch.rand(100) + 0.01\n",
    "            non_init_word_embeds[current_index] = init_value\n",
    "            non_init_word_embeds_all[current_index_all] = init_value\n",
    "            current_index += 1 #since its a dict, out indexes can be negative probably\n",
    "            current_index_all += 1\n",
    "word_indexes_en.update(non_init_word_indexes)\n",
    "word_indexes_all_en.update(non_init_word_indexes_all)\n",
    "word_embeddings_en.update(non_init_word_embeds)\n",
    "word_embeddings_all_en.update(non_init_word_embeds_all)\n",
    "current_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.8400e-02,  1.3640e-01,  2.0190e-01, -1.0000e-04,  2.5780e-01,\n",
       "         5.8900e-02, -3.3300e-01,  8.5000e-02,  4.0000e-04,  9.3100e-02,\n",
       "         2.1940e-01,  2.1300e-01,  2.7970e-01,  1.6510e-01,  1.2840e-01,\n",
       "         3.9200e-01,  2.2390e-01, -1.2000e-01,  8.8700e-02,  5.3600e-02,\n",
       "         1.0650e-01,  5.2000e-03,  3.7510e-01, -4.2400e-02,  2.3600e-02,\n",
       "         3.2150e-01,  2.0360e-01,  1.8630e-01, -4.3200e-02,  2.0160e-01,\n",
       "         2.7880e-01,  1.1990e-01, -3.5000e-03, -4.0400e-02,  3.5560e-01,\n",
       "         1.2460e-01, -1.3600e-01, -5.6900e-02,  6.0600e-02,  2.7280e-01,\n",
       "         9.6900e-02,  4.6000e-02, -3.1650e-01,  4.2000e-02, -4.2800e-02,\n",
       "         2.5300e-02,  4.3100e-02,  2.2600e-02, -8.8500e-02, -2.5040e-01,\n",
       "        -7.8100e-02, -2.4600e-02, -1.8000e-02,  3.9900e-02, -1.1280e-01,\n",
       "         2.4850e-01, -1.3300e-02,  8.5500e-02,  9.0000e-03,  1.9590e-01,\n",
       "         2.5100e-02, -1.6200e-02,  2.3690e-01, -7.2000e-03, -1.3850e-01,\n",
       "        -1.9370e-01,  1.1420e-01,  1.6320e-01, -8.0000e-03, -1.4670e-01,\n",
       "        -4.1170e-01, -6.1200e-02, -5.0300e-02, -9.8200e-02,  6.9000e-03,\n",
       "        -2.5300e-02, -2.0300e-02, -1.4540e-01, -1.8810e-01,  2.7210e-01,\n",
       "        -1.5770e-01,  2.3540e-01, -1.9300e-02, -1.1330e-01, -7.2300e-02,\n",
       "         8.6100e-02,  1.7340e-01, -6.4300e-02, -1.0390e-01,  2.4990e-01,\n",
       "        -2.8510e-01,  1.1540e-01, -1.0450e-01,  3.1310e-01,  1.7760e-01,\n",
       "        -1.9210e-01, -1.4140e-01, -6.0700e-02, -1.4310e-01,  1.0810e-01])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings_en[word_indexes_en['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.8400e-02,  1.3640e-01,  2.0190e-01, -1.0000e-04,  2.5780e-01,\n",
       "         5.8900e-02, -3.3300e-01,  8.5000e-02,  4.0000e-04,  9.3100e-02,\n",
       "         2.1940e-01,  2.1300e-01,  2.7970e-01,  1.6510e-01,  1.2840e-01,\n",
       "         3.9200e-01,  2.2390e-01, -1.2000e-01,  8.8700e-02,  5.3600e-02,\n",
       "         1.0650e-01,  5.2000e-03,  3.7510e-01, -4.2400e-02,  2.3600e-02,\n",
       "         3.2150e-01,  2.0360e-01,  1.8630e-01, -4.3200e-02,  2.0160e-01,\n",
       "         2.7880e-01,  1.1990e-01, -3.5000e-03, -4.0400e-02,  3.5560e-01,\n",
       "         1.2460e-01, -1.3600e-01, -5.6900e-02,  6.0600e-02,  2.7280e-01,\n",
       "         9.6900e-02,  4.6000e-02, -3.1650e-01,  4.2000e-02, -4.2800e-02,\n",
       "         2.5300e-02,  4.3100e-02,  2.2600e-02, -8.8500e-02, -2.5040e-01,\n",
       "        -7.8100e-02, -2.4600e-02, -1.8000e-02,  3.9900e-02, -1.1280e-01,\n",
       "         2.4850e-01, -1.3300e-02,  8.5500e-02,  9.0000e-03,  1.9590e-01,\n",
       "         2.5100e-02, -1.6200e-02,  2.3690e-01, -7.2000e-03, -1.3850e-01,\n",
       "        -1.9370e-01,  1.1420e-01,  1.6320e-01, -8.0000e-03, -1.4670e-01,\n",
       "        -4.1170e-01, -6.1200e-02, -5.0300e-02, -9.8200e-02,  6.9000e-03,\n",
       "        -2.5300e-02, -2.0300e-02, -1.4540e-01, -1.8810e-01,  2.7210e-01,\n",
       "        -1.5770e-01,  2.3540e-01, -1.9300e-02, -1.1330e-01, -7.2300e-02,\n",
       "         8.6100e-02,  1.7340e-01, -6.4300e-02, -1.0390e-01,  2.4990e-01,\n",
       "        -2.8510e-01,  1.1540e-01, -1.0450e-01,  3.1310e-01,  1.7760e-01,\n",
       "        -1.9210e-01, -1.4140e-01, -6.0700e-02, -1.4310e-01,  1.0810e-01])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeds_tensor_en = torch.zeros(len(word_embeddings_en), 100)\n",
    "for i in word_embeddings_en:\n",
    "    word_embeds_tensor_en[i] = word_embeddings_en[i]\n",
    "word_embeds_tensor_en.size()\n",
    "\n",
    "#init_embeds_tensor = torch.stack(tuple([word_embeddings_en[i] for i in word_embeddings_en]))\n",
    "#init_embeds_tensor = torch.tensor(dtype=torch.long)\n",
    "#for i in range(len(word_embeddings_en)):\n",
    "#    init_embeds_tensor.stack\n",
    "#word_embeds_en = nn.Embedding(len(word_embeddings_en), 100)\n",
    "#lookup_tensor = torch.tensor([word_indexes_en[\"the\"]], dtype=torch.long)\n",
    "#word_embeds_en(lookup_tensor)\n",
    "word_embeds_tensor_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency list of training xpos\n",
    "train_xpos_en = {}\n",
    "for i in ud_ewt_train:\n",
    "    for j in i['xpos']:\n",
    "        if j not in train_xpos_en:\n",
    "            train_xpos_en[j] = 1\n",
    "        else:\n",
    "            train_xpos_en[j] += 1\n",
    "train_xpos_en = dict(sorted(train_xpos_en.items(), key=lambda item: item[1], reverse=True))\n",
    "#train_xpos_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency list of training upos\n",
    "train_upos_en = {}\n",
    "for i in ud_ewt_train:\n",
    "    for j in i['upos']:\n",
    "        if upos_map[j] not in train_upos_en:\n",
    "            train_upos_en[upos_map[j]] = 1\n",
    "        else:\n",
    "            train_upos_en[upos_map[j]] += 1\n",
    "train_upos_en = dict(sorted(train_upos_en.items(), key=lambda item: item[1], reverse=True))\n",
    "#train_upos_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency list of training deps\n",
    "train_deps_en = {}\n",
    "for i in ud_ewt_train:\n",
    "    for j in i['deprel']:\n",
    "        if j not in train_deps_en:\n",
    "            train_deps_en[j] = 1\n",
    "        else:\n",
    "            train_deps_en[j] += 1\n",
    "train_deps_en = dict(sorted(train_deps_en.items(), key=lambda item: item[1], reverse=True))\n",
    "#train_deps_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([57, 100])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomly initialize pos and label embeddings (Chen and Manning 3.2) \n",
    "#xpos_tags is full list of arcs in conllu documentation. also has NULL appended\n",
    "xpos_tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '$', ':', ',', '.', \"``\", '’’', '#', '-LRB-', '-RRB-', 'HYPH', 'NFP', 'SYM', 'PUNC', '_', 'ROOT', 'NULL']\n",
    "index = 0\n",
    "xpos_indexes = {}\n",
    "for j in set([i for i in train_xpos_en]) | set(xpos_tags): #combine with all xpos found in training for more complete list of xpos\n",
    "    xpos_indexes[j] = index\n",
    "    index += 1\n",
    "xpos_embeddings_en = {}\n",
    "for i in xpos_indexes:\n",
    "    xpos_embeddings_en[xpos_indexes[i]] = (-0.01 - 0.01) * torch.rand(100) + 0.01\n",
    "    \n",
    "xpos_embeds_tensor_en = torch.zeros(len(xpos_indexes), 100)\n",
    "for i in xpos_embeddings_en:\n",
    "    xpos_embeds_tensor_en[i] = xpos_embeddings_en[i]\n",
    "xpos_embeds_tensor_en.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 100])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same as xpos but for arc labels\n",
    "arc_labels = ['nsubj', 'nsubj:pass', 'nsubj:outer', 'obj', 'iobj', 'csubj', 'csubj:pass', 'csubj:outer', 'ccomp', 'xcomp', 'obl', 'obl:npmod', 'obl:tmod', 'advcl', 'advcl:relcl', 'advmod', 'vocative', 'discourse', 'expl', 'aux', 'aux:pass', 'cop', 'mark', 'nummod', 'appos', 'nmod', 'nmod:npmod', 'nmod:tmod', 'nmod:poss', 'acl', 'acl:relcl', 'amod', 'det', 'det:predet', 'compound', 'compound:prt', 'fixed', 'flat', 'flat:foreign', 'goeswith', 'conj', 'cc', 'cc:preconj', 'case', 'list', 'dislocated', 'parataxis', 'orphan', 'reparandum', 'root', 'punct', 'dep', 'NULL']\n",
    "#arc_labels is generic list of arcs. also has NULL appended\n",
    "index = 0\n",
    "arc_indexes = {}\n",
    "for j in set([i for i in train_deps_en]) | set(arc_labels):\n",
    "    arc_indexes[j] = index\n",
    "    index += 1\n",
    "arc_embeddings_en = {}\n",
    "for i in arc_indexes:\n",
    "    arc_embeddings_en[arc_indexes[i]] = (-0.01 - 0.01) * torch.rand(100) + 0.01\n",
    "    \n",
    "arc_embeds_tensor_en = torch.zeros(len(arc_indexes), 100)\n",
    "for i in arc_embeddings_en:\n",
    "    arc_embeds_tensor_en[i] = arc_embeddings_en[i]\n",
    "arc_embeds_tensor_en.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeds = nn.Embedding.from_pretrained(word_embeds_tensor_en)\n",
    "xpos_embeds = nn.Embedding.from_pretrained(xpos_embeds_tensor_en)\n",
    "arc_embeds = nn.Embedding.from_pretrained(arc_embeds_tensor_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8400e-02,  1.3640e-01,  2.0190e-01, -1.0000e-04,  2.5780e-01,\n",
       "          5.8900e-02, -3.3300e-01,  8.5000e-02,  4.0000e-04,  9.3100e-02,\n",
       "          2.1940e-01,  2.1300e-01,  2.7970e-01,  1.6510e-01,  1.2840e-01,\n",
       "          3.9200e-01,  2.2390e-01, -1.2000e-01,  8.8700e-02,  5.3600e-02,\n",
       "          1.0650e-01,  5.2000e-03,  3.7510e-01, -4.2400e-02,  2.3600e-02,\n",
       "          3.2150e-01,  2.0360e-01,  1.8630e-01, -4.3200e-02,  2.0160e-01,\n",
       "          2.7880e-01,  1.1990e-01, -3.5000e-03, -4.0400e-02,  3.5560e-01,\n",
       "          1.2460e-01, -1.3600e-01, -5.6900e-02,  6.0600e-02,  2.7280e-01,\n",
       "          9.6900e-02,  4.6000e-02, -3.1650e-01,  4.2000e-02, -4.2800e-02,\n",
       "          2.5300e-02,  4.3100e-02,  2.2600e-02, -8.8500e-02, -2.5040e-01,\n",
       "         -7.8100e-02, -2.4600e-02, -1.8000e-02,  3.9900e-02, -1.1280e-01,\n",
       "          2.4850e-01, -1.3300e-02,  8.5500e-02,  9.0000e-03,  1.9590e-01,\n",
       "          2.5100e-02, -1.6200e-02,  2.3690e-01, -7.2000e-03, -1.3850e-01,\n",
       "         -1.9370e-01,  1.1420e-01,  1.6320e-01, -8.0000e-03, -1.4670e-01,\n",
       "         -4.1170e-01, -6.1200e-02, -5.0300e-02, -9.8200e-02,  6.9000e-03,\n",
       "         -2.5300e-02, -2.0300e-02, -1.4540e-01, -1.8810e-01,  2.7210e-01,\n",
       "         -1.5770e-01,  2.3540e-01, -1.9300e-02, -1.1330e-01, -7.2300e-02,\n",
       "          8.6100e-02,  1.7340e-01, -6.4300e-02, -1.0390e-01,  2.4990e-01,\n",
       "         -2.8510e-01,  1.1540e-01, -1.0450e-01,  3.1310e-01,  1.7760e-01,\n",
       "         -1.9210e-01, -1.4140e-01, -6.0700e-02, -1.4310e-01,  1.0810e-01]])"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeds(torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'s1': 'has',\n",
       "  's2': '[ROOT]',\n",
       "  's3': 'NULL',\n",
       "  'b1': 'NULL',\n",
       "  'b2': 'NULL',\n",
       "  'b3': 'NULL',\n",
       "  'lc1s1': 'He',\n",
       "  'lc2s1': 'control',\n",
       "  'lc1s2': 'NULL',\n",
       "  'lc2s2': 'NULL',\n",
       "  'rc1s1': '.',\n",
       "  'rc2s1': 'NULL',\n",
       "  'rc1s2': 'NULL',\n",
       "  'rc2s2': 'NULL',\n",
       "  'lc1lc1s1': 'NULL',\n",
       "  'lc1lc1s2': 'NULL',\n",
       "  'rc1rc1s1': 'NULL',\n",
       "  'rc1rc1s2': 'NULL'},\n",
       " {'s1': 'VBZ',\n",
       "  's2': 'NULL',\n",
       "  's3': 'NULL',\n",
       "  'b1': 'NULL',\n",
       "  'b2': 'NULL',\n",
       "  'b3': 'NULL',\n",
       "  'lc1s1': 'PRP',\n",
       "  'lc2s1': 'NN',\n",
       "  'lc1s2': 'NULL',\n",
       "  'lc2s2': 'NULL',\n",
       "  'rc1s1': '.',\n",
       "  'rc2s1': 'NULL',\n",
       "  'rc1s2': 'NULL',\n",
       "  'rc2s2': 'NULL',\n",
       "  'lc1lc1s1': 'NULL',\n",
       "  'lc1lc1s2': 'NULL',\n",
       "  'rc1rc1s1': 'NULL',\n",
       "  'rc1rc1s2': 'NULL'},\n",
       " {'lc1s1': 'nsubj',\n",
       "  'lc2s1': 'obj',\n",
       "  'lc1s2': 'NULL',\n",
       "  'lc2s2': 'NULL',\n",
       "  'rc1s1': 'punct',\n",
       "  'rc2s1': 'NULL',\n",
       "  'rc1s2': 'NULL',\n",
       "  'rc2s2': 'NULL',\n",
       "  'lc1lc1s1': 'NULL',\n",
       "  'lc1lc1s2': 'NULL',\n",
       "  'rc1rc1s1': 'NULL',\n",
       "  'rc1rc1s2': 'NULL'})"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20134"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_indexes_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nsubj',\n",
       " 'obj',\n",
       " 'NULL',\n",
       " 'NULL',\n",
       " 'punct',\n",
       " 'NULL',\n",
       " 'NULL',\n",
       " 'NULL',\n",
       " 'NULL',\n",
       " 'NULL',\n",
       " 'NULL',\n",
       " 'NULL']"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[test_S[2][i] for i in test_S[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyParser(nn.Module):\n",
    "    def __init__(self, word_embeddings, pos_embeddings, arc_embeddings, bias=torch.zeros(100)):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.pos_embeddings = pos_embeddings\n",
    "        self.arc_embeddings = arc_embeddings\n",
    "        self.bias = bias\n",
    "        self.weights_1w = torch.ones(18)\n",
    "        self.weights_1t = torch.ones(18)\n",
    "        self.weights_1l = torch.ones(12)\n",
    "        self.weights_2 = torch.ones(3,100)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "    \n",
    "    #for unlabeled, the arc label can be whatever\n",
    "    def forward(self, stack, buffer, arcs):\n",
    "        h = torch.zeros(3)\n",
    "        for i in range(3): #['left_arc', 'right_arc', 'shift']\n",
    "            config = self.copy_config(stack, buffer, arcs)\n",
    "            if i == 0:\n",
    "                result = left_arc(config[0], config[1], config[2], 'NULL', False)\n",
    "            elif i == 1:\n",
    "                result = right_arc(config[0], config[1], config[2], 'NULL', False)\n",
    "            else:\n",
    "                result = shift(config[0], config[1], config[2], False)\n",
    "            if result:\n",
    "                print(config[0], config[1], config[2])\n",
    "                S = featurize_configuration(config[0], config[1], config[2])\n",
    "                #print(S[1])\n",
    "                x_w = word_embeds(torch.tensor([word_indexes_en[S[0][j]] for j in S[0]]))\n",
    "                x_t = xpos_embeds(torch.tensor([xpos_indexes[S[1][j]] for j in S[1]]))\n",
    "                x_l = arc_embeds(torch.tensor([arc_indexes[S[2][j]] for j in S[2]]))\n",
    "                print(x_w, x_t, x_l)\n",
    "                #(a+b+c+d)^3 = (a(a+b+c+d)^2 + b(a+b+c+d)^2 + c(a+b+c+d)^2 + d(a+b+c+d)^2)\n",
    "                Wwxw = torch.matmul(self.weights_1w, x_w)\n",
    "                Wtxt = torch.matmul(self.weights_1t, x_t)\n",
    "                Wlxl = torch.matmul(self.weights_1l, x_l)\n",
    "                print(Wwxw, Wtxt, Wlxl)\n",
    "                #Wwxw + Wtxt + Wlxl + self.bias\n",
    "                h_i = 0\n",
    "                for a in [Wwxw, Wtxt, Wlxl, self.bias]:\n",
    "                    for b in [Wwxw, Wtxt, Wlxl, self.bias]:\n",
    "                        for c in [Wwxw, Wtxt, Wlxl, self.bias]:\n",
    "                            h_i += a*b*c\n",
    "                #print(h_i)\n",
    "                h[i] = torch.dot(h_i, self.weights_2[i])\n",
    "        #print(h)\n",
    "        after_softmax = self.softmax(h)\n",
    "        #print(after_softmax)\n",
    "        print(torch.max(after_softmax))\n",
    "    \n",
    "    def copy_config(self, stack, buffer, arcs):\n",
    "        new_stack = copy.deepcopy(stack)\n",
    "        new_buffer = copy.deepcopy(buffer)\n",
    "        new_arcs = copy.deepcopy(arcs)\n",
    "        return new_stack, new_buffer, new_arcs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('control', 'NN')] [('.', '.')] [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('NULL', (('control', 'NN'), ('good', 'JJ')))]\n",
      "tensor([[-3.4580e-01,  4.8350e-01,  3.7410e-01,  ..., -3.3580e-01,\n",
      "         -5.2020e-01,  2.0170e-01],\n",
      "        [-1.0800e-02,  6.6000e-02,  3.2820e-01,  ..., -4.2300e-02,\n",
      "         -1.8210e-01,  9.0900e-02],\n",
      "        [ 5.1104e-03, -3.4017e-04, -4.1354e-03,  ..., -5.3025e-04,\n",
      "         -6.6370e-03,  1.9116e-03],\n",
      "        ...,\n",
      "        [-1.6907e-03, -6.9148e-03,  2.4643e-03,  ...,  4.4364e-04,\n",
      "          2.1513e-03,  1.2247e-03],\n",
      "        [-1.6907e-03, -6.9148e-03,  2.4643e-03,  ...,  4.4364e-04,\n",
      "          2.1513e-03,  1.2247e-03],\n",
      "        [-1.6907e-03, -6.9148e-03,  2.4643e-03,  ...,  4.4364e-04,\n",
      "          2.1513e-03,  1.2247e-03]]) tensor([[ 0.0001,  0.0067,  0.0097,  ..., -0.0058, -0.0052,  0.0030],\n",
      "        [-0.0073, -0.0012,  0.0014,  ..., -0.0016, -0.0006, -0.0097],\n",
      "        [ 0.0034,  0.0052,  0.0029,  ...,  0.0065, -0.0039, -0.0055],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0005,  0.0083,  ...,  0.0036,  0.0052, -0.0012],\n",
      "        [ 0.0011, -0.0005,  0.0083,  ...,  0.0036,  0.0052, -0.0012],\n",
      "        [ 0.0011, -0.0005,  0.0083,  ...,  0.0036,  0.0052, -0.0012]]) tensor([[-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [ 0.0076,  0.0075, -0.0073,  ..., -0.0010, -0.0003, -0.0017],\n",
      "        ...,\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089]])\n",
      "tensor([-8.2698e-01,  7.3000e-01,  3.0743e-01, -2.9498e-01,  1.9169e+00,\n",
      "        -6.4536e-01, -1.4841e+00, -5.4210e-01,  1.5610e-01, -2.7778e-01,\n",
      "         7.9495e-02, -1.3435e-01,  1.0213e+00,  2.5821e-01,  7.7417e-01,\n",
      "         6.7292e-01, -3.8075e-01, -1.1678e+00, -1.3076e-01,  1.0341e+00,\n",
      "        -8.0862e-01,  1.6485e-01,  2.1907e+00, -8.4473e-01,  3.0487e-01,\n",
      "         4.5026e-01, -4.4630e-01,  1.0314e+00,  1.8067e-01,  6.8203e-01,\n",
      "         2.0571e+00,  8.0614e-01,  1.5765e-01, -2.7902e-01,  1.2591e+00,\n",
      "         1.4924e+00, -1.7310e-01,  3.9670e-01,  1.8310e+00,  1.3358e-01,\n",
      "         1.5393e+00,  1.1786e-01, -1.0124e+00,  1.2722e+00,  5.6795e-01,\n",
      "         1.7465e+00,  2.1273e+00, -3.3325e-02, -2.3634e+00, -2.8812e-01,\n",
      "        -1.9352e-01, -3.1326e-01, -5.1195e-01, -4.8185e-01,  5.4054e-02,\n",
      "         1.5681e+00,  6.7297e-01,  1.1335e+00,  5.3627e-01,  2.7525e-01,\n",
      "        -6.6362e-02, -2.5888e-04,  2.3532e-01, -1.1786e+00,  5.2954e-01,\n",
      "        -8.2823e-01, -5.9390e-02,  1.5749e+00,  2.8100e-01, -2.8562e-02,\n",
      "        -1.8754e+00, -5.1707e-01, -4.9088e-01, -4.7506e-01, -3.1077e-01,\n",
      "        -1.1320e+00,  1.3542e+00, -5.4388e-01, -7.7300e-01,  5.4555e-01,\n",
      "        -1.1170e+00,  1.9469e+00, -5.0704e-01, -9.3716e-01, -7.7483e-01,\n",
      "        -6.0394e-01,  1.2766e+00, -5.1908e-01,  1.7354e-02, -2.9947e-01,\n",
      "        -6.3278e-01,  9.5300e-03,  1.1943e+00,  1.2525e+00,  2.6343e+00,\n",
      "        -1.3425e+00, -3.0360e-01,  6.0531e-01,  9.0996e-01,  3.9800e-02]) tensor([ 0.0023,  0.0177,  0.1132, -0.0716, -0.0931, -0.0620,  0.0961, -0.0933,\n",
      "        -0.0415,  0.0163, -0.0363,  0.0645,  0.1158,  0.0411,  0.1398, -0.0186,\n",
      "         0.0155, -0.0427,  0.0947,  0.0336, -0.0908,  0.0637, -0.0024, -0.0200,\n",
      "        -0.0473, -0.0704, -0.0619, -0.0894, -0.0817,  0.0539,  0.0485,  0.1365,\n",
      "        -0.0795, -0.0027,  0.0339,  0.0587,  0.0160,  0.0767,  0.0362,  0.0896,\n",
      "         0.1248, -0.0171,  0.0665, -0.0711, -0.0031, -0.0516,  0.0518,  0.0985,\n",
      "         0.0953,  0.0728,  0.0856, -0.0042,  0.0630, -0.0019, -0.0873, -0.0135,\n",
      "        -0.0291, -0.0641,  0.0124, -0.0135, -0.0051,  0.0841, -0.1115, -0.1064,\n",
      "        -0.0686, -0.0526, -0.0339,  0.1184, -0.0666, -0.0740,  0.1096,  0.1143,\n",
      "        -0.0670, -0.0695, -0.0506,  0.0616,  0.0029, -0.0026,  0.0938,  0.0558,\n",
      "        -0.0192,  0.0539, -0.0123,  0.0666,  0.0128,  0.0910,  0.0970, -0.0376,\n",
      "        -0.1363,  0.0806, -0.1148, -0.0665,  0.0321,  0.0417,  0.0854,  0.0203,\n",
      "        -0.0330,  0.0462,  0.0484, -0.0307]) tensor([-0.0135, -0.0510,  0.0677,  0.0540,  0.0117,  0.0836,  0.0246,  0.0593,\n",
      "        -0.0214,  0.0790,  0.0399,  0.1173,  0.0354, -0.0154, -0.1129, -0.0898,\n",
      "        -0.1032, -0.0373, -0.0029,  0.0208, -0.0002, -0.0308,  0.0439, -0.0981,\n",
      "         0.0191,  0.0805,  0.0055,  0.0353,  0.0056, -0.0973, -0.0030, -0.0461,\n",
      "         0.0779,  0.0767,  0.0756,  0.0836, -0.0325, -0.0525,  0.0417,  0.0890,\n",
      "        -0.0401, -0.1005,  0.0171,  0.0713,  0.1037,  0.0952, -0.0888,  0.0869,\n",
      "         0.0660, -0.0800, -0.0304, -0.0944,  0.0001,  0.0075, -0.0426, -0.0952,\n",
      "        -0.0743,  0.0195, -0.0906, -0.0544,  0.0271, -0.0215, -0.0420, -0.0936,\n",
      "        -0.0227,  0.0771,  0.0406, -0.0588,  0.0238,  0.0158,  0.0462, -0.0485,\n",
      "        -0.0036, -0.0300, -0.0308, -0.0052, -0.0998,  0.1024, -0.1053, -0.1179,\n",
      "         0.0815,  0.0707, -0.0026, -0.0349, -0.0778, -0.0330, -0.0951, -0.0836,\n",
      "         0.0150, -0.0935,  0.0214,  0.0145,  0.0775, -0.0418,  0.0518, -0.0933,\n",
      "         0.0773,  0.0846, -0.0619,  0.0960])\n",
      "[('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('good', 'JJ')] [('.', '.')] [('nsubj', (('has', 'VBZ'), ('He', 'PRP'))), ('NULL', (('good', 'JJ'), ('control', 'NN')))]\n",
      "tensor([[-0.2799, -0.1372, -0.2523,  ..., -0.1035, -0.0982, -0.2648],\n",
      "        [-0.0108,  0.0660,  0.3282,  ..., -0.0423, -0.1821,  0.0909],\n",
      "        [ 0.0051, -0.0003, -0.0041,  ..., -0.0005, -0.0066,  0.0019],\n",
      "        ...,\n",
      "        [-0.0017, -0.0069,  0.0025,  ...,  0.0004,  0.0022,  0.0012],\n",
      "        [-0.0017, -0.0069,  0.0025,  ...,  0.0004,  0.0022,  0.0012],\n",
      "        [-0.0017, -0.0069,  0.0025,  ...,  0.0004,  0.0022,  0.0012]]) tensor([[ 0.0014,  0.0089,  0.0028,  ...,  0.0055,  0.0099, -0.0070],\n",
      "        [-0.0073, -0.0012,  0.0014,  ..., -0.0016, -0.0006, -0.0097],\n",
      "        [ 0.0034,  0.0052,  0.0029,  ...,  0.0065, -0.0039, -0.0055],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0005,  0.0083,  ...,  0.0036,  0.0052, -0.0012],\n",
      "        [ 0.0011, -0.0005,  0.0083,  ...,  0.0036,  0.0052, -0.0012],\n",
      "        [ 0.0011, -0.0005,  0.0083,  ...,  0.0036,  0.0052, -0.0012]]) tensor([[-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [ 0.0076,  0.0075, -0.0073,  ..., -0.0010, -0.0003, -0.0017],\n",
      "        ...,\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089]])\n",
      "tensor([-8.2698e-01,  7.3000e-01,  3.0743e-01, -2.9498e-01,  1.9169e+00,\n",
      "        -6.4536e-01, -1.4841e+00, -5.4210e-01,  1.5610e-01, -2.7778e-01,\n",
      "         7.9495e-02, -1.3435e-01,  1.0213e+00,  2.5821e-01,  7.7417e-01,\n",
      "         6.7292e-01, -3.8075e-01, -1.1678e+00, -1.3076e-01,  1.0341e+00,\n",
      "        -8.0862e-01,  1.6485e-01,  2.1907e+00, -8.4473e-01,  3.0487e-01,\n",
      "         4.5026e-01, -4.4630e-01,  1.0314e+00,  1.8067e-01,  6.8203e-01,\n",
      "         2.0571e+00,  8.0614e-01,  1.5765e-01, -2.7902e-01,  1.2591e+00,\n",
      "         1.4924e+00, -1.7310e-01,  3.9670e-01,  1.8310e+00,  1.3358e-01,\n",
      "         1.5393e+00,  1.1786e-01, -1.0124e+00,  1.2722e+00,  5.6795e-01,\n",
      "         1.7465e+00,  2.1273e+00, -3.3325e-02, -2.3634e+00, -2.8812e-01,\n",
      "        -1.9352e-01, -3.1326e-01, -5.1195e-01, -4.8185e-01,  5.4054e-02,\n",
      "         1.5681e+00,  6.7297e-01,  1.1335e+00,  5.3627e-01,  2.7525e-01,\n",
      "        -6.6362e-02, -2.5887e-04,  2.3532e-01, -1.1786e+00,  5.2954e-01,\n",
      "        -8.2823e-01, -5.9390e-02,  1.5749e+00,  2.8100e-01, -2.8562e-02,\n",
      "        -1.8754e+00, -5.1707e-01, -4.9088e-01, -4.7506e-01, -3.1077e-01,\n",
      "        -1.1320e+00,  1.3542e+00, -5.4388e-01, -7.7300e-01,  5.4555e-01,\n",
      "        -1.1170e+00,  1.9469e+00, -5.0704e-01, -9.3716e-01, -7.7483e-01,\n",
      "        -6.0394e-01,  1.2766e+00, -5.1908e-01,  1.7354e-02, -2.9947e-01,\n",
      "        -6.3278e-01,  9.5300e-03,  1.1943e+00,  1.2525e+00,  2.6343e+00,\n",
      "        -1.3425e+00, -3.0360e-01,  6.0531e-01,  9.0996e-01,  3.9800e-02]) tensor([ 0.0023,  0.0177,  0.1132, -0.0716, -0.0931, -0.0620,  0.0961, -0.0933,\n",
      "        -0.0415,  0.0163, -0.0363,  0.0645,  0.1158,  0.0411,  0.1398, -0.0186,\n",
      "         0.0155, -0.0427,  0.0947,  0.0336, -0.0908,  0.0637, -0.0024, -0.0200,\n",
      "        -0.0473, -0.0704, -0.0619, -0.0894, -0.0817,  0.0539,  0.0485,  0.1365,\n",
      "        -0.0795, -0.0027,  0.0339,  0.0587,  0.0160,  0.0767,  0.0362,  0.0896,\n",
      "         0.1248, -0.0171,  0.0665, -0.0711, -0.0031, -0.0516,  0.0518,  0.0985,\n",
      "         0.0953,  0.0728,  0.0856, -0.0042,  0.0630, -0.0019, -0.0873, -0.0135,\n",
      "        -0.0291, -0.0641,  0.0124, -0.0135, -0.0051,  0.0841, -0.1115, -0.1064,\n",
      "        -0.0686, -0.0526, -0.0339,  0.1184, -0.0666, -0.0740,  0.1096,  0.1143,\n",
      "        -0.0670, -0.0695, -0.0506,  0.0616,  0.0029, -0.0026,  0.0938,  0.0558,\n",
      "        -0.0192,  0.0539, -0.0123,  0.0666,  0.0128,  0.0910,  0.0970, -0.0376,\n",
      "        -0.1363,  0.0806, -0.1148, -0.0665,  0.0321,  0.0417,  0.0854,  0.0203,\n",
      "        -0.0330,  0.0462,  0.0484, -0.0307]) tensor([-0.0135, -0.0510,  0.0677,  0.0540,  0.0117,  0.0836,  0.0246,  0.0593,\n",
      "        -0.0214,  0.0790,  0.0399,  0.1173,  0.0354, -0.0154, -0.1129, -0.0898,\n",
      "        -0.1032, -0.0373, -0.0029,  0.0208, -0.0002, -0.0308,  0.0439, -0.0981,\n",
      "         0.0191,  0.0805,  0.0055,  0.0353,  0.0056, -0.0973, -0.0030, -0.0461,\n",
      "         0.0779,  0.0767,  0.0756,  0.0836, -0.0325, -0.0525,  0.0417,  0.0890,\n",
      "        -0.0401, -0.1005,  0.0171,  0.0713,  0.1037,  0.0952, -0.0888,  0.0869,\n",
      "         0.0660, -0.0800, -0.0304, -0.0944,  0.0001,  0.0075, -0.0426, -0.0952,\n",
      "        -0.0743,  0.0195, -0.0906, -0.0544,  0.0271, -0.0215, -0.0420, -0.0936,\n",
      "        -0.0227,  0.0771,  0.0406, -0.0588,  0.0238,  0.0158,  0.0462, -0.0485,\n",
      "        -0.0036, -0.0300, -0.0308, -0.0052, -0.0998,  0.1024, -0.1053, -0.1179,\n",
      "         0.0815,  0.0707, -0.0026, -0.0349, -0.0778, -0.0330, -0.0951, -0.0836,\n",
      "         0.0150, -0.0935,  0.0214,  0.0145,  0.0775, -0.0418,  0.0518, -0.0933,\n",
      "         0.0773,  0.0846, -0.0619,  0.0960])\n",
      "[('[ROOT]', 'ROOT'), ('has', 'VBZ'), ('good', 'JJ'), ('control', 'NN'), ('.', '.')] [] [('nsubj', (('has', 'VBZ'), ('He', 'PRP')))]\n",
      "tensor([[ 4.0012e-03,  9.4217e-03,  5.6940e-03,  ...,  6.6208e-03,\n",
      "          6.4821e-03,  2.8918e-03],\n",
      "        [-3.4580e-01,  4.8350e-01,  3.7410e-01,  ..., -3.3580e-01,\n",
      "         -5.2020e-01,  2.0170e-01],\n",
      "        [-2.7990e-01, -1.3720e-01, -2.5230e-01,  ..., -1.0350e-01,\n",
      "         -9.8200e-02, -2.6480e-01],\n",
      "        ...,\n",
      "        [-1.6907e-03, -6.9148e-03,  2.4643e-03,  ...,  4.4364e-04,\n",
      "          2.1513e-03,  1.2247e-03],\n",
      "        [-1.6907e-03, -6.9148e-03,  2.4643e-03,  ...,  4.4364e-04,\n",
      "          2.1513e-03,  1.2247e-03],\n",
      "        [-1.6907e-03, -6.9148e-03,  2.4643e-03,  ...,  4.4364e-04,\n",
      "          2.1513e-03,  1.2247e-03]]) tensor([[-0.0088, -0.0001,  0.0009,  ...,  0.0042, -0.0069, -0.0011],\n",
      "        [ 0.0001,  0.0067,  0.0097,  ..., -0.0058, -0.0052,  0.0030],\n",
      "        [ 0.0014,  0.0089,  0.0028,  ...,  0.0055,  0.0099, -0.0070],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0005,  0.0083,  ...,  0.0036,  0.0052, -0.0012],\n",
      "        [ 0.0011, -0.0005,  0.0083,  ...,  0.0036,  0.0052, -0.0012],\n",
      "        [ 0.0011, -0.0005,  0.0083,  ...,  0.0036,  0.0052, -0.0012]]) tensor([[-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        ...,\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089],\n",
      "        [-0.0019, -0.0053,  0.0068,  ...,  0.0078, -0.0056,  0.0089]])\n",
      "tensor([-0.6471,  0.2520,  0.1645, -0.1186,  0.3802, -0.0349, -0.4999, -0.0157,\n",
      "        -0.0611,  0.2427, -0.0435,  0.2266, -0.0203,  0.2623,  0.2236,  1.0072,\n",
      "         0.0109, -1.0257,  0.0987,  0.4477,  0.0840,  0.7548,  0.4730, -0.4241,\n",
      "         0.3901,  0.1208, -0.2408,  0.4728,  0.0395, -0.2745,  0.7320,  0.0263,\n",
      "         0.2966, -0.3796,  0.0190,  1.0036, -0.3380,  0.2617,  0.3573,  0.2520,\n",
      "         0.4222, -0.0383, -0.8886,  0.2803,  0.4330,  0.8280,  0.7535, -0.3039,\n",
      "        -0.7576, -0.5602, -0.2358, -0.5005,  0.1498,  0.2446,  0.0358,  0.5200,\n",
      "         0.1779,  0.1369,  0.3788,  0.0057,  0.2231,  0.0301,  0.5674, -0.5220,\n",
      "        -0.3415, -0.5712, -0.2258, -0.0066,  0.2768, -0.1787, -0.7799, -0.0359,\n",
      "        -0.6241, -0.1718, -0.0701, -0.6665,  0.0585, -0.0795, -0.7784, -0.0634,\n",
      "        -0.0917,  0.8353, -0.2710, -0.7686, -0.4178,  0.3008,  0.4153,  0.0928,\n",
      "        -0.1523,  0.0857, -0.0837, -0.0954,  0.2003,  0.6670,  0.7603, -0.2543,\n",
      "        -0.1350, -0.4260, -0.5796, -0.0418]) tensor([ 0.0093,  0.0083,  0.1385, -0.0890, -0.1176, -0.0582,  0.1070, -0.1319,\n",
      "        -0.0429,  0.0448, -0.0282,  0.0922,  0.1324,  0.0509,  0.1423, -0.0287,\n",
      "         0.0042, -0.0501,  0.1073,  0.0616, -0.1205,  0.1023, -0.0035, -0.0243,\n",
      "        -0.0423, -0.0787, -0.0689, -0.1166, -0.0844,  0.0524,  0.0503,  0.1421,\n",
      "        -0.1120,  0.0108,  0.0540,  0.0630,  0.0194,  0.0825,  0.0578,  0.1145,\n",
      "         0.1462, -0.0325,  0.0744, -0.0924, -0.0269, -0.0523,  0.0693,  0.1325,\n",
      "         0.0962,  0.0690,  0.0927, -0.0182,  0.0967,  0.0154, -0.1040, -0.0074,\n",
      "        -0.0399, -0.0920,  0.0189, -0.0115, -0.0066,  0.1003, -0.1504, -0.1437,\n",
      "        -0.1052, -0.0298, -0.0450,  0.1322, -0.1007, -0.1060,  0.1362,  0.1160,\n",
      "        -0.0754, -0.0787, -0.0492,  0.0627,  0.0100, -0.0008,  0.1188,  0.0642,\n",
      "        -0.0267,  0.0662, -0.0062,  0.0775, -0.0093,  0.0988,  0.1111, -0.0325,\n",
      "        -0.1572,  0.1230, -0.1393, -0.0709,  0.0409,  0.0694,  0.1098,  0.0182,\n",
      "        -0.0609,  0.0577,  0.0757, -0.0231]) tensor([-0.0229, -0.0639,  0.0819,  0.0561,  0.0193,  0.0989,  0.0240,  0.0602,\n",
      "        -0.0180,  0.0788,  0.0474,  0.1174,  0.0358, -0.0243, -0.1192, -0.1060,\n",
      "        -0.1044, -0.0444, -0.0011,  0.0302,  0.0057, -0.0371,  0.0470, -0.1142,\n",
      "         0.0237,  0.0786,  0.0040,  0.0419, -0.0042, -0.1170, -0.0034, -0.0468,\n",
      "         0.0863,  0.0772,  0.0860,  0.0854, -0.0446, -0.0511,  0.0416,  0.0987,\n",
      "        -0.0363, -0.1064,  0.0288,  0.0796,  0.1155,  0.1133, -0.0941,  0.1056,\n",
      "         0.0655, -0.0794, -0.0412, -0.1078, -0.0070,  0.0017, -0.0534, -0.1121,\n",
      "        -0.0741,  0.0222, -0.0945, -0.0627,  0.0303, -0.0324, -0.0418, -0.0927,\n",
      "        -0.0182,  0.0855,  0.0389, -0.0733,  0.0315,  0.0247,  0.0446, -0.0524,\n",
      "        -0.0130, -0.0283, -0.0245, -0.0098, -0.1150,  0.1083, -0.1158, -0.1198,\n",
      "         0.0861,  0.0857, -0.0112, -0.0304, -0.0809, -0.0281, -0.1078, -0.0834,\n",
      "         0.0057, -0.0926,  0.0159,  0.0205,  0.0774, -0.0377,  0.0567, -0.1092,\n",
      "         0.0761,  0.0934, -0.0671,  0.1066])\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "test_stack4 = [(\"[ROOT]\", \"ROOT\")]\n",
    "test_buffer4 = [(test_sent['tokens'][i], test_sent['xpos'][i]) for i in range(len(test_sent['tokens']))]\n",
    "test_arcs4 = []\n",
    "shift(test_stack4, test_buffer4, test_arcs4, False)\n",
    "shift(test_stack4, test_buffer4, test_arcs4, False)\n",
    "left_arc(test_stack4, test_buffer4, test_arcs4, 'nsubj', False)\n",
    "shift(test_stack4, test_buffer4, test_arcs4, False)\n",
    "shift(test_stack4, test_buffer4, test_arcs4, False)\n",
    "model = MyParser(word_embeds, xpos_embeds, arc_embeds)\n",
    "model(test_stack4, test_buffer4, test_arcs4)\n",
    "#left_arc(test_stack, test_buffer, test_arcs, 'amod', False)\n",
    "#right_arc(test_stack, test_buffer, test_arcs, 'obj', False)\n",
    "#hift(test_stack, test_buffer, test_arcs, False)\n",
    "#right_arc(test_stack, test_buffer, test_arcs, 'punct')\n",
    "#test_S = featurize_configuration(test_stack, test_buffer, test_arcs)\n",
    "#print(test_S)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
